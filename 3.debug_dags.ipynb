{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug DAGs\n",
    "\n",
    "In this notebook, we debug our DAGs without running them through Apache Airflow, that is, by pasing them to EMR cluster directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('airflow/dags/lib')\n",
    "import emrspark_lib as emrs\n",
    "import configparser\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "if config['App']['STOCKS'] == '':\n",
    "    STOCKS = []\n",
    "else:\n",
    "    STOCKS = json.loads(config.get('App', 'STOCKS').replace(\"'\", '\"'))\n",
    "\n",
    "ec2, emr, iam = emrs.get_boto_clients(config['AWS']['REGION_NAME'], config=config)\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = emrs.get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = emrs.get_first_available_subnet(ec2, VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found Security Group: sg-031830023db36a998 in vpc vpc-0a0f40087989cc73e (us-east-1).\n",
      "INFO:root:Found Security Group: sg-01ee208f3283ea93b in vpc vpc-0a0f40087989cc73e (us-east-1).\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): iam.amazonaws.com\n",
      "INFO:root:Role EMR_EC2_DefaultRole is ready\n",
      "INFO:root:Role EMR_DefaultRole is ready\n",
      "INFO:root:Instance Profile EMR_EC2_DefaultRole is ready\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): elasticmapreduce.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "master_sg_id = emrs.create_security_group(ec2, '{}SG'.format(CLUSTER_NAME),\n",
    "    'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = emrs.create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME),\n",
    "    'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "\n",
    "keypair = emrs.create_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))\n",
    "\n",
    "emrs.create_default_roles(iam)\n",
    "emrs.wait_for_roles(iam)\n",
    "\n",
    "cluster_id = emrs.create_emr_cluster(emr, CLUSTER_NAME,\n",
    "                master_sg_id,\n",
    "                slave_sg_id,\n",
    "                keypair['KeyName'], SUBNET_ID,\n",
    "                release_label='emr-5.28.1',\n",
    "                num_core_nodes=3)\n",
    "cluster_dns = emrs.get_cluster_dns(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cells below do not need to be run sequentially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "#         .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#         .config(\"spark.eventLog.dir\" \"test_data/spark-logs\") \\\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv('s3a://short-interest-effect/data/raw/short_interests_nasdaq', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Query with the index\n",
    "tail = sqlContext.sql(\"\"\"SELECT * FROM df ORDER BY index DESC limit 5\"\"\")\n",
    "tail.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_headers = emrs.create_spark_session(cluster_dns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = emrs.spark_url(cluster_dns, location='/sessions')\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(emrs.spark_url(cluster_dns, location='/sessions'))\n",
    "spark_sessions = response.json()['sessions']\n",
    "print(spark_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response_headers = {'Date': 'Mon, 27 Jan 2020 15:37:51 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/31/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
    "emrs.track_spark_job(cluster_dns, job_response_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': 's3a://short-interest-effect',\n",
    "    'TABLE_STOCK_INFO_NASDAQ': '/data/raw/stock_info_nasdaq',\n",
    "    'TABLE_STOCK_INFO_NYSE': '/data/raw/stock_info_nyse',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_stock_info.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_id)\n",
    "print(master_sg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_si = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-02-04',\n",
    "#     'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "#     'STOCKS': STOCKS,\n",
    "    'LIMIT': None,\n",
    "    'STOCKS': [],\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all spark sessions\n",
      "INFO:root:Killed idle spark session id 1\n",
      "INFO:root:Sent spark session creation command to http://ec2-54-89-71-25.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Fri, 07 Feb 2020 14:45:30 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/2', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:{'id': 2, 'name': None, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Fri, 07 Feb 2020 14:45:30 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/2', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Fri, 07 Feb 2020 14:46:09 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/2/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: waiting\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "\n",
      "INFO:root:Final job Status: waiting\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 0.03984857541342897\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.03984857541342897, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 0.19137278342299263\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.19137278342299263, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 0.3519625423391114\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.3519625423391114, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 0.5197250448296473\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.5197250448296473, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 0.7192667862123929\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.7192667862123929, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 0.8792588164973102\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.8792588164973102, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: running\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'ename': 'ImportError',\n",
      "            'evalue': 'Pandas >= 0.19.2 must be installed; however, it was not '\n",
      "                      'found.',\n",
      "            'execution_count': 0,\n",
      "            'status': 'error',\n",
      "            'traceback': ['Traceback (most recent call last):\\n',\n",
      "                          '  File \"<stdin>\", line 195, in '\n",
      "                          'pull_short_interests\\n',\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", '\n",
      "                          'line 2076, in toPandas\\n'\n",
      "                          '    require_minimum_pandas_version()\\n',\n",
      "                          '  File '\n",
      "                          '\"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", '\n",
      "                          'line 129, in require_minimum_pandas_version\\n'\n",
      "                          '    \"it was not found.\" % minimum_pandas_version)\\n',\n",
      "                          'ImportError: Pandas >= 0.19.2 must be installed; '\n",
      "                          'however, it was not found.\\n']},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Log from the cluster:\n",
      "20/02/07 14:45:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/02/07 14:45:50 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/02/07 14:46:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "\n",
      "INFO:root:Final job Status: available\n",
      "INFO:root:Statement exception: Pandas >= 0.19.2 must be installed; however, it was not found.\n",
      "INFO:root:Traceback (most recent call last):\n",
      "\n",
      "INFO:root:  File \"<stdin>\", line 195, in pull_short_interests\n",
      "\n",
      "INFO:root:  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2076, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "\n",
      "INFO:root:  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 129, in require_minimum_pandas_version\n",
      "    \"it was not found.\" % minimum_pandas_version)\n",
      "\n",
      "INFO:root:ImportError: Pandas >= 0.19.2 must be installed; however, it was not found.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Stopped because the final job status was \"error\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-41f918c7dbdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mhelperspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'airflow/dags/etl/helpers.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfinal_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_spark_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_dns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_response_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0memrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_spark_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_dns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/jay/DATA/learn/udacity-dend/short_interest_effect/airflow/dags/lib/emrspark_lib.py\u001b[0m in \u001b[0;36mtrack_spark_job\u001b[0;34m(master_dns, job_response_headers, port, sleep_seconds)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstatement_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'traceback'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stopped because the final job status was \"error\".'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_job_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Stopped because the final job status was \"error\"."
     ]
    }
   ],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests.py',\n",
    "        args=args_si,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=10)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_p = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-02-04',\n",
    "    'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "    'STOCKS': STOCKS,\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "    'URL': \"http://app.quotemedia.com/quotetools/getHistoryDownload.csv?&webmasterId=501&startDay={sd}&startMonth={sm}&startYear={sy}&endDay={ed}&endMonth={em}&endYear={ey}&isRanged=true&symbol={sym}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices.py',\n",
    "        args=args_p,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check - Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_qs = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'YESTERDAY_DATE': '2020-02-03',\n",
    "            'STOCKS': STOCKS,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests_quality.py',\n",
    "        args=args_qs,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "20/02/06 04:22:11 WARN DAG: NASDAQ short interest latest date: 2020-02-03\n",
    "20/02/06 04:25:16 WARN DAG: Last 5 dates: [Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check - Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_qp = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'YESTERDAY_DATE': '2020-02-03',\n",
    "            'STOCKS': STOCKS,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "            'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices_quality.py',\n",
    "        args=args_qp,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=10)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_c = {\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine.py',\n",
    "        args=args_c,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Quantopian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_cq = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS'],\n",
    "            'TABLE_SHORT_ANALYSIS_QUANTOPIAN': config['App']['TABLE_SHORT_ANALYSIS_QUANTOPIAN']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/prepare_for_quantopian.py',\n",
    "        args=args_cq,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=10)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")\n",
    "        \n",
    "# Update files to public readable:\n",
    "\n",
    "bucket = config['App']['DB_HOST'].split('/')[-1]\n",
    "\n",
    "key = config['App']['TABLE_SHORT_ANALYSIS'][1:]+'.csv'\n",
    "(boto3\n",
    " .session\n",
    " .Session(region_name='us-east-1')\n",
    " .resource('s3')\n",
    " .Object(bucket, key)\n",
    " .copy_from(CopySource={'Bucket': bucket,\n",
    "                        'Key': key},\n",
    "            MetadataDirective=\"REPLACE\",\n",
    "            ContentType=\"text/csv\")\n",
    ")\n",
    "(boto3\n",
    " .session\n",
    " .Session(region_name='us-east-1')\n",
    " .resource('s3')\n",
    " .Object(bucket, key)\n",
    " .Acl()\n",
    " .put(ACL='public-read'))\n",
    "\n",
    "key = config['App']['TABLE_SHORT_ANALYSIS_QUANTOPIAN'][1:]+'.csv'\n",
    "(boto3\n",
    " .session\n",
    " .Session(region_name='us-east-1')\n",
    " .resource('s3')\n",
    " .Object(bucket, key)\n",
    " .copy_from(CopySource={'Bucket': bucket,\n",
    "                        'Key': key},\n",
    "            MetadataDirective=\"REPLACE\",\n",
    "            ContentType=\"text/csv\")\n",
    ")\n",
    "(boto3\n",
    " .session\n",
    " .Session(region_name='us-east-1')\n",
    " .resource('s3')\n",
    " .Object(bucket, key)\n",
    " .Acl()\n",
    " .put(ACL='public-read'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-2445HR9SR059T has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-2445HR9SR059T has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n"
     ]
    }
   ],
   "source": [
    "emrs.delete_cluster(emr, cluster_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

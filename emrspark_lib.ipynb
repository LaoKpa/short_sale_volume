{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `emrspark_lib` plugin creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import subprocess\n",
    "import json\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "REGION_NAME = config['AWS']['REGION_NAME']\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "\n",
    "# When empty, use the first available VPC\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "# When empty, use the first available subnet\n",
    "# NOTE: Subnet must have an internet gateway within its routes.\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "# If access and secret keys are empty, use the one stored by the OS.\n",
    "\n",
    "if config['AWS']['AWS_ACCESS_KEY_ID'] != '' and config['AWS']['AWS_SECRET_ACCESS_KEY'] != '':    \n",
    "    ec2 = boto3.client('ec2', region_name=REGION_NAME,\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                      )\n",
    "    emr = boto3.client('emr', region_name=REGION_NAME,\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                      )\n",
    "    iam = boto3.client('iam', region_name=REGION_NAME,\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                      )\n",
    "else:\n",
    "    ec2 = boto3.client('ec2', region_name=REGION_NAME)\n",
    "    emr = boto3.client('emr', region_name=REGION_NAME)\n",
    "    iam = boto3.client('iam', region_name=REGION_NAME)\n",
    "    \n",
    "    \n",
    "def get_first_available_vpc(ec2_client):\n",
    "    return ec2.describe_vpcs().get('Vpcs', [{}])[0].get('VpcId', '')\n",
    "\n",
    "def get_first_available_subnet(ec2_client, vpc_id):\n",
    "    return ec2.describe_subnets(Filters=[{'Name': 'vpc-id', 'Values': [vpc_id]}, {'Name': 'state', 'Values': ['available']}])['Subnets'][0].get('SubnetId', '')\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = get_first_available_subnet(ec2, VPC_ID)\n",
    "    \n",
    "# def create_spark_session():\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "#         .getOrCreate()\n",
    "#     return spark\n",
    "\n",
    "# emrlib.create_cluster()\n",
    "print('vpc:', VPC_ID)\n",
    "print('subnet:', SUBNET_ID)\n",
    "print('region:', ec2.meta.region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing getting default ip address.\n",
    "ip = requests.get('https://api.ipify.org').text\n",
    "print('My public IP address is:', ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_security_group(ec2_client, name, desc, vpc_id, ip=None):\n",
    "    \"\"\" Create a security group\n",
    "    Args:\n",
    "        - ec2_client (boto3.EC2.Client): EC2 client object.\n",
    "        - name (string): Name of Security Group\n",
    "        - desc (string): Description of Security Group\n",
    "        - vpc_id (string): Name of VPC. If empty, use the first available VPC\n",
    "        - ip (string): The IP address of this machine. Only this machine can connect to the cluster.\n",
    "                       If empty, use https://api.ipify.org service to get public IP address.\n",
    "    Return:\n",
    "    \n",
    "        dict: {\n",
    "            'KeyFingerprint': 'string',\n",
    "            'KeyMaterial': 'string',\n",
    "            'KeyName': 'string',\n",
    "            'KeyPairId': 'string'\n",
    "        }\n",
    "    \"\"\"\n",
    "    region = ec2_client.meta.region_name\n",
    "    security_group_id = None\n",
    "    \n",
    "    try:\n",
    "        # Do not create if we found an existing Security Group\n",
    "        response = ec2_client.describe_security_groups(\n",
    "            Filters=[\n",
    "                {'Name':'group-name', 'Values': [name]}\n",
    "            ]\n",
    "        )\n",
    "        groups = response['SecurityGroups']\n",
    "        security_group_id = groups[0]['GroupId']\n",
    "\n",
    "        if ip is None:\n",
    "            ip = requests.get('https://api.ipify.org').text\n",
    "\n",
    "        if len(groups) > 0:\n",
    "            # Update the rule to use the new IP address\n",
    "            \n",
    "            ip_permissions = groups[0]['IpPermissions']\n",
    "            for ip_permission in ip_permissions:\n",
    "                # Delete all rules that listens to TCP port 8998\n",
    "                if ip_permission[\"IpProtocol\"] == 'tcp' and ip_permission[\"FromPort\"] == 8998 and ip_permission[\"FromPort\"] == 8998:\n",
    "                    cidr_ip = ip_permission['IpRanges'][0]['CidrIp']\n",
    "                    revoke_status = ec2_client.revoke_security_group_ingress(\n",
    "                        GroupId=security_group_id,\n",
    "                        IpPermissions=[\n",
    "                            {'IpProtocol': 'tcp',\n",
    "                             'FromPort': 8998,\n",
    "                             'ToPort': 8998,\n",
    "                             'IpRanges': [{'CidrIp': cidr_ip}]\n",
    "                            }\n",
    "                        ])\n",
    "            \n",
    "            # Create a new inbound rule that listens to this machine's IP\n",
    "            data = ec2_client.authorize_security_group_ingress(\n",
    "                GroupId=security_group_id,\n",
    "                IpPermissions=[\n",
    "                    {'IpProtocol': 'tcp',\n",
    "                     'FromPort': 8998,\n",
    "                     'ToPort': 8998,\n",
    "                     'IpRanges': [{'CidrIp': '{}/32'.format(ip)}]}\n",
    "                ])\n",
    "            return groups[0]['GroupId']\n",
    "        else:\n",
    "            response = ec2_client.create_security_group(GroupName=name,\n",
    "                                                 Description=desc,\n",
    "                                                 VpcId=vpc_id)\n",
    "            security_group_id = response['GroupId']\n",
    "#             print('Security Group Created %s in vpc %s (%s).' % (security_group_id, vpc_id, region))\n",
    "\n",
    "            data = ec2_client.authorize_security_group_ingress(\n",
    "                GroupId=security_group_id,\n",
    "                IpPermissions=[\n",
    "                    {'IpProtocol': 'tcp',\n",
    "                     'FromPort': 8998,\n",
    "                     'ToPort': 8998,\n",
    "                     'IpRanges': [{'CidrIp': '{}/32'.format(ip)}]}\n",
    "                ])\n",
    "#             print('Ingress Successfully Set %s' % data)\n",
    "            return security_group_id\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "        return security_group_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_sg_id = create_security_group(ec2, '{}SG'.format(CLUSTER_NAME), 'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME), 'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create EMR Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testvar = None\n",
    "\n",
    "def recreate_default_roles(iam_client):\n",
    "    # Recreate default roles\n",
    "    try:\n",
    "        iam_client.remove_role_from_instance_profile(InstanceProfileName='EMR_EC2_DefaultRole', RoleName='EMR_EC2_DefaultRole')\n",
    "        iam_client.delete_instance_profile(InstanceProfileName='EMR_EC2_DefaultRole')\n",
    "        iam_client.detach_role_policy(RoleName='EMR_EC2_DefaultRole', PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n",
    "        iam_client.delete_role(RoleName='EMR_EC2_DefaultRole')\n",
    "        iam_client.detach_role_policy(RoleName='EMR_DefaultRole', PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n",
    "        iam_client.delete_role(RoleName='EMR_DefaultRole')\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        pass\n",
    "    return subprocess.check_output(['aws', 'emr', 'create-default-roles'])\n",
    "\n",
    "\n",
    "def recreate_key_pair(ec2_client, key_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - ec2_client (boto3.EC2.Client): EC2 client object.\n",
    "        - key_name (string): Name of key, usually 'xxx_pem'\n",
    "    Return:\n",
    "    \n",
    "        dict: {\n",
    "            'KeyFingerprint': 'string',\n",
    "            'KeyMaterial': 'string',\n",
    "            'KeyName': 'string',\n",
    "            'KeyPairId': 'string'\n",
    "        }\n",
    "    \"\"\"\n",
    "    ec2_client.delete_key_pair(KeyName=key_name)\n",
    "    keypair = ec2_client.create_key_pair(KeyName=key_name)\n",
    "    return keypair\n",
    "\n",
    "\n",
    "class ClusterError(Exception):\n",
    "    def __init__(self, last_guess):\n",
    "        self.last_guess = last_guess\n",
    "            \n",
    "def create_emr_cluster(emr_client, cluster_name, master_sg, slave_sg, keypair_name, subnet_id, job_flow_role='EMR_EC2_DefaultRole', service_role='EMR_DefaultRole', release_label='emr-5.9.0',\n",
    "                   master_instance_type='m3.xlarge', num_core_nodes=3, core_node_instance_type='m3.xlarge'):\n",
    "    \"\"\" Create an EMR cluster\n",
    "    Args:\n",
    "        - subnet_id (string): If empty, use first available VPC (VPC is inferred from Security Groups)\n",
    "    \"\"\"\n",
    "    # Avoid recreating cluster\n",
    "    clusters = emr_client.list_clusters(ClusterStates=['STARTING', 'RUNNING', 'WAITING', 'BOOTSTRAPPING'])\n",
    "    active_clusters = [i for i in clusters['Clusters'] if i['Name'] == cluster_name]\n",
    "    if len(active_clusters) > 0:\n",
    "        return active_clusters[0]['Id']\n",
    "    else:\n",
    "        # Create cluster\n",
    "        cluster_response = emr_client.run_job_flow(\n",
    "            Name=cluster_name,\n",
    "            ReleaseLabel=release_label,\n",
    "            Instances={\n",
    "                'InstanceGroups': [\n",
    "                    {\n",
    "                        'Name': \"Master nodes\",\n",
    "                        'Market': 'ON_DEMAND',\n",
    "                        'InstanceRole': 'MASTER',\n",
    "                        'InstanceType': master_instance_type,\n",
    "                        'InstanceCount': 1\n",
    "                    },\n",
    "                    {\n",
    "                        'Name': \"Slave nodes\",\n",
    "                        'Market': 'ON_DEMAND',\n",
    "                        'InstanceRole': 'CORE',\n",
    "                        'InstanceType': core_node_instance_type,\n",
    "                        'InstanceCount': num_core_nodes\n",
    "                    }\n",
    "                ],\n",
    "                'KeepJobFlowAliveWhenNoSteps': True,\n",
    "                'Ec2SubnetId': subnet_id,\n",
    "                'Ec2KeyName' : keypair_name,\n",
    "                'EmrManagedMasterSecurityGroup': master_sg,\n",
    "                'EmrManagedSlaveSecurityGroup': slave_sg\n",
    "            },\n",
    "            VisibleToAllUsers=True,\n",
    "            JobFlowRole=job_flow_role,\n",
    "            ServiceRole=service_role,\n",
    "            Applications=[\n",
    "                { 'Name': 'hadoop' },\n",
    "                { 'Name': 'spark' },\n",
    "                { 'Name': 'hive' },\n",
    "                { 'Name': 'livy' },\n",
    "                { 'Name': 'zeppelin' }\n",
    "            ]\n",
    "        )\n",
    "        cluster_id = cluster_response['JobFlowId']\n",
    "        cluster_state = emr_client.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "        if cluster_state != 'STARTING':\n",
    "            reason = emr_client.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['StateChangeReason']\n",
    "            raise Exception(\"Cluster error: {} - {}\".format(reason['Code'], reason['Message']))\n",
    "        return cluster_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypair = recreate_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_default_roles(iam)\n",
    "print(iam.get_role(RoleName='EMR_EC2_DefaultRole'))\n",
    "print(iam.get_role(RoleName='EMR_DefaultRole'))\n",
    "# Wait a bit until the roles are ready, otherwise we'd get Failed to authorize instance profile arn.../instance-profile/EMR_EC2_DefaultRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = create_emr_cluster(emr, CLUSTER_NAME, master_sg_id, slave_sg_id, keypair['KeyName'], SUBNET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr.describe_cluster(ClusterId=cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "Wait until the cluster is in WAITING state and then create a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_status(cluster_id):\n",
    "    cluster = emr.describe_cluster(ClusterId=cluster_id)\n",
    "    return cluster['Cluster']['Status']['State']\n",
    "\n",
    "\n",
    "def is_cluster_ready(cluster_id):\n",
    "    return get_cluster_status(cluster_id) == 'WAITING'\n",
    "\n",
    "\n",
    "def get_cluster_dns(cluster_id):\n",
    "    cluster = emr.describe_cluster(ClusterId=cluster_id)\n",
    "    return cluster['Cluster']['MasterPublicDnsName']\n",
    "\n",
    "\n",
    "def spark_url(master_dns, location='', port=8998):\n",
    "    \"\"\"Get spark session url.\"\"\"\n",
    "    return 'http://{}:{}{}'.format(master_dns, port, location)\n",
    "\n",
    "    \n",
    "def create_spark_session(master_dns):\n",
    "    # 8998 is the port on which the Livy server runs\n",
    "    host = spark_url(master_dns)\n",
    "    data = {'kind': 'pyspark', \n",
    "            \"conf\" : {\"spark.jars.packages\" : \"saurfang:spark-sas7bdat:2.0.0-s_2.11\",\n",
    "                      \"spark.driver.extraJavaOptions\" : \"-Dlog4jspark.root.logger=WARN,console\"\n",
    "                     }\n",
    "           }\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "    logging.info(response.json())\n",
    "    return response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_cluster_ready(cluster_id):\n",
    "    cluster_dns = get_cluster_dns(cluster_id)\n",
    "    ss_headers = create_spark_session(cluster_dns)\n",
    "    print(ss_headers)\n",
    "else:\n",
    "    print(\"Cluster is not ready (status is {}), run this code cell again later.\".format(get_cluster_status(cluster_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Spark jobs\n",
    "\n",
    "We will try pulling some stock market data from Quandl and QuoteMedia. Stock names are available here:\n",
    "\n",
    "- NASDAQ: https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download\n",
    "- AMEX: https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=amex&render=download\n",
    "- NYSE: https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Quandl request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_map = {\n",
    "    'nasdaq': 'FNSQ',\n",
    "    'nyse': 'FNYX'\n",
    "}\n",
    "\n",
    "def get_short_interests_pandas(exchange, ticker, api_key):\n",
    "    response = requests.get(\"https://www.quandl.com/api/v3/datasets/FINRA/{}_{}?api_key={}\".format(exchange, ticker, api_key))\n",
    "    if response.status_code == 200:\n",
    "        response_obj = response.json()\n",
    "        return pd.DataFrame(data=response_obj['dataset']['data'], columns=response_obj['dataset']['column_names'])\n",
    "    else:\n",
    "        raise Exception(\"Error when connecting to Quandl API.\")\n",
    "\n",
    "df = get_short_interests_pandas('FNYX', 'FB', config['Quandl']['API_KEY'])\n",
    "print(df.describe())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quandl request through Spark\n",
    "\n",
    "[Spark cannot pull data from URL.](https://stackoverflow.com/questions/29741082/how-to-access-a-web-url-using-a-spark-context/29741462) The other alternative is to download data to S3 from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def get_short_interests(spark, exchange, ticker, api_key):\n",
    "    url = \"https://www.quandl.com/api/v3/datasets/FINRA/{}_{}?api_key={}\".format(exchange, ticker, api_key)\n",
    "#     spark.sparkContext.addFile(url)\n",
    "#     response = spark.read.json(\"file://{}\".format(SparkFiles.get(\"{}_{}\".format(exchange, ticker))))\n",
    "#     print(response)\n",
    "    result = requests.get(url).json()\n",
    "    df = spark.createDataFrame(result['dataset']['data'], result['dataset']['column_names'])\n",
    "    df.createOrReplaceTempView('test')\n",
    "    table = spark.sql(\"SELECT * FROM test\")\n",
    "    table_path = \"test_table\"\n",
    "    table.write.mode('overwrite').parquet(table_path)\n",
    "\n",
    "df = get_short_interests(spark, 'FNYX', 'FB', config['Quandl']['API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Spark job to download from Quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def wait_for_spark(session_url, session_headers):\n",
    "    \"\"\"Wait until status is idle\"\"\"\n",
    "    status = ''\n",
    "    while status != 'idle':\n",
    "        response = requests.get(session_url, headers=session_headers)\n",
    "        status = response.json()['state']\n",
    "        time.sleep(5)\n",
    "        logging.info(\"Spark session status: {}\".format(status))\n",
    "        \n",
    "    \n",
    "def submit_spark_job(session_url, session_headers, code):\n",
    "    wait_for_spark(session_url, session_headers)\n",
    "    statements_url = \"{}/statements\".format(session_url)\n",
    "    job = {'code': code}\n",
    "    response = requests.post(statements_url, data=json.dumps(job),\n",
    "                             headers={'Content-Type': 'application/json'})\n",
    "    logging.info(response.json)\n",
    "    return response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = 'FNYX'\n",
    "ticker = 'FB'\n",
    "code = \"\"\"\n",
    "import requests\n",
    "url = \"https://www.quandl.com/api/v3/datasets/FINRA/{exchange}_{ticker}?api_key={quandl_api}\"\n",
    "result = requests.get(url).json()\n",
    "df = spark.createDataFrame(result['dataset']['data'], result['dataset']['column_names'])\n",
    "df.createOrReplaceTempView('test')\n",
    "table = spark.sql(\"SELECT * FROM test\")\n",
    "table_path = \"s3://short-interest-effect/data/test_table\"\n",
    "table.write.mode('overwrite').parquet(table_path)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "code = code.format(exchange=exchange, ticker=ticker, quandl_api=config['Quandl']['API_KEY'])\n",
    "job_response = submit_spark_job(spark_url(cluster_dns, location=ss_headers['location']), ss_headers, code)\n",
    "print(job_response.status_code)\n",
    "print(job_response.headers)\n",
    "print(job_response.json()['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response.headers['location'].split('/statements', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Spark job status\n",
    "\n",
    "The following code can be run several times to check the result of the above statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_status = ''\n",
    "statements_url = spark_url(cluster_dns, location=job_response.headers['location'])\n",
    "statements_response = requests.get(statements_url, headers={'content-Type': 'application/json'})\n",
    "print(statements_response)\n",
    "print(statements_response.headers)\n",
    "print(\"State:\", statements_response.json()['state'])\n",
    "print(\"Output:\\n\",statements_response.json()['output'])\n",
    "\n",
    "session_url = spark_url(cluster_dns, location=job_response.headers['location'].split('/statements', 1)[0])\n",
    "log_url = session_url + '/log'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def track_spark_job(master_dns, response_headers):\n",
    "    statement_status = ''\n",
    "    host = 'http://' + master_dns + ':8998'\n",
    "    session_url = host + response_headers['location'].split('/statements', 1)[0]\n",
    "    print(session_url)\n",
    "    # Poll the status of the submitted scala code\n",
    "    while statement_status != 'available':\n",
    "        # If a statement takes longer than a few milliseconds to execute, Livy returns early and provides a statement URL that can be polled until it is complete:\n",
    "        statement_url = host + response_headers['location']\n",
    "        statement_response = requests.get(statement_url, headers={'Content-Type': 'application/json'})\n",
    "        statement_status = statement_response.json()['state']\n",
    "        logging.info('Statement status: ' + statement_status)\n",
    "        logging.info(statement_response.json())\n",
    "        if 'progress' in statement_response.json():\n",
    "            logging.info('Progress: ' + str(statement_response.json()['progress']))\n",
    "        time.sleep(10)\n",
    "    final_statement_status = statement_response.json()['output']['status']\n",
    "    if final_statement_status == 'error':\n",
    "        logging.info('Statement exception: ' + statement_response.json()['output']['evalue'])\n",
    "        for trace in statement_response.json()['output']['traceback']:\n",
    "            logging.info(trace)\n",
    "        raise ValueError('Final Statement Status: ' + final_statement_status)\n",
    "    \n",
    "    # Get the logs\n",
    "    lines = requests.get(session_url + '/log', \n",
    "                        headers={'Content-Type': 'application/json'}).json()['log']\n",
    "    logging.info('Final Statement Status: ' + final_statement_status)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_spark_job(cluster_dns, ss_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_spark_session(session_url):\n",
    "    requests.delete(session_url, headers={'Content-Type': 'application/json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_spark_session(spark_url(cluster_dns, location=ss_headers['location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_cluster(emr_client, cluster_id):\n",
    "    try:\n",
    "        response = emr_client.terminate_job_flows(JobFlowIds=[cluster_id])\n",
    "        print('Cluster {} Deleted'.format(cluster_id))\n",
    "    except ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cluster(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Key Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2.delete_key_pair(KeyName=keypair['KeyName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr.describe_cluster(ClusterId=cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cluster_terminated(cluster_id):\n",
    "    cluster = emr.describe_cluster(ClusterId=cluster_id)\n",
    "    return 'TERMINATED' in cluster['Cluster']['Status']['State']\n",
    "\n",
    "def delete_security_group(ec2, sgid):\n",
    "    region=ec2.meta.region_name\n",
    "    # Delete security group\n",
    "    try:\n",
    "        ec2res = boto3.resource('ec2')\n",
    "        sg = ec2res.SecurityGroup(sgid)\n",
    "        if len(sg.ip_permissions) > 0:\n",
    "            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n",
    "        response = ec2.delete_security_group(GroupId=sgid)\n",
    "        print('Security Group {} Deleted'.format(sgid))\n",
    "    except ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_cluster_terminated(cluster_id):\n",
    "    # delete_security_group(ec2, sg)\n",
    "    delete_security_group(ec2, master_sg_id)\n",
    "    delete_security_group(ec2, slave_sg_id)\n",
    "else:\n",
    "    cluster = emr.describe_cluster(ClusterId=cluster_id)\n",
    "    state = cluster['Cluster']['Status']['State']\n",
    "    print(\"Cluster is not terminated. If it is terminating, wait until the status is TERMINATED. Current cluster state: {}\".format(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- EMR creation that works: https://github.com/dai-dao/udacity-data-engineering-capstone/blob/master/dags/lib/emr_lib.py\n",
    "- On Security Group Creation and Deletion: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/ec2-example-security-group.html\n",
    "- On how to recreate EMR_EC2_DefaultRole: https://aws.amazon.com/premiumsupport/knowledge-center/emr-default-role-invalid/\n",
    "- Using Apache Livy with Spark on EMR: https://aws.amazon.com/blogs/big-data/orchestrate-apache-spark-applications-using-aws-step-functions-and-apache-livy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

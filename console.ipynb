{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console\n",
    "\n",
    "In this notebook, we debug our DAGs without running them through Apache Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('airflow/dags/lib')\n",
    "import emrspark_lib as emrs\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "if config['App']['STOCKS'] == '':\n",
    "    STOCKS = []\n",
    "else:\n",
    "    STOCKS = json.loads(config.get('App', 'STOCKS').replace(\"'\", '\"'))\n",
    "\n",
    "ec2, emr, iam = emrs.get_boto_clients(config['AWS']['REGION_NAME'], config=config)\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = emrs.get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = emrs.get_first_available_subnet(ec2, VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_sg_id = emrs.create_security_group(ec2, '{}SG'.format(CLUSTER_NAME),\n",
    "    'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = emrs.create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME),\n",
    "    'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "\n",
    "keypair = emrs.create_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))\n",
    "\n",
    "emrs.create_default_roles(iam)\n",
    "emrs.wait_for_roles(iam)\n",
    "\n",
    "cluster_id = emrs.create_emr_cluster(emr, CLUSTER_NAME,\n",
    "                master_sg_id,\n",
    "                slave_sg_id,\n",
    "                keypair['KeyName'], SUBNET_ID,\n",
    "                release_label='emr-5.28.1',\n",
    "                num_core_nodes=1)\n",
    "cluster_dns = emrs.get_cluster_dns(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "#         .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#         .config(\"spark.eventLog.dir\" \"test_data/spark-logs\") \\\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('s3a://short-interest-effect/data/raw/short_interests_nasdaq').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_headers = emrs.create_spark_session(cluster_dns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = emrs.spark_url(cluster_dns, location='/sessions')\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(emrs.spark_url(cluster_dns, location='/sessions'))\n",
    "spark_sessions = response.json()['sessions']\n",
    "print(spark_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response_headers = {'Date': 'Mon, 27 Jan 2020 15:37:51 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/31/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
    "emrs.track_spark_job(cluster_dns, job_response_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': 's3a://short-interest-effect',\n",
    "    'TABLE_STOCK_INFO_NASDAQ': '/data/raw/stock_info_nasdaq',\n",
    "    'TABLE_STOCK_INFO_NYSE': '/data/raw/stock_info_nyse',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_stock_info.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_id)\n",
    "print(master_sg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_si = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "#     'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "#     'STOCKS': STOCKS,\n",
    "    'LIMIT': 1,\n",
    "    'STOCKS': [],\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests.py',\n",
    "        args=args_si,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_p = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "    'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "    'STOCKS': STOCKS,\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "    'URL': \"http://app.quotemedia.com/quotetools/getHistoryDownload.csv?&webmasterId=501&startDay={sd}&startMonth={sm}&startYear={sy}&endDay={ed}&endMonth={em}&endYear={ey}&isRanged=true&symbol={sym}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices.py',\n",
    "        args=args_p,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_q = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'STOCKS': STOCKS,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "            'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices_quality.py',\n",
    "        args=args_q,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_c = {\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine.py',\n",
    "        args=args_c,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.delete_cluster(emr, cluster_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console\n",
    "\n",
    "In this notebook, we debug our DAGs without running then through Apache Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('airflow/dags/lib')\n",
    "import emrspark_lib as emrs\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "if config['App']['STOCKS'] == '':\n",
    "    STOCKS = []\n",
    "else:\n",
    "    STOCKS = json.loads(config.get('App', 'STOCKS').replace(\"'\", '\"'))\n",
    "\n",
    "ec2, emr, iam = emrs.get_boto_clients(config['AWS']['REGION_NAME'], config=config)\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = emrs.get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = emrs.get_first_available_subnet(ec2, VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Output of recreate_default_roles:\n",
      "[{'Role': {'Path': '/', 'RoleName': 'EMR_EC2_DefaultRole', 'RoleId': 'AROAZ7YWVG745AFBQOE57', 'Arn': 'arn:aws:iam::686705424377:role/EMR_EC2_DefaultRole', 'CreateDate': '2020-01-27T11:21:58Z', 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}, 'RolePolicy': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Resource': '*', 'Action': ['cloudwatch:*', 'dynamodb:*', 'ec2:Describe*', 'elasticmapreduce:Describe*', 'elasticmapreduce:ListBootstrapActions', 'elasticmapreduce:ListClusters', 'elasticmapreduce:ListInstanceGroups', 'elasticmapreduce:ListInstances', 'elasticmapreduce:ListSteps', 'kinesis:CreateStream', 'kinesis:DeleteStream', 'kinesis:DescribeStream', 'kinesis:GetRecords', 'kinesis:GetShardIterator', 'kinesis:MergeShards', 'kinesis:PutRecord', 'kinesis:SplitShard', 'rds:Describe*', 's3:*', 'sdb:*', 'sns:*', 'sqs:*', 'glue:CreateDatabase', 'glue:UpdateDatabase', 'glue:DeleteDatabase', 'glue:GetDatabase', 'glue:GetDatabases', 'glue:CreateTable', 'glue:UpdateTable', 'glue:DeleteTable', 'glue:GetTable', 'glue:GetTables', 'glue:GetTableVersions', 'glue:CreatePartition', 'glue:BatchCreatePartition', 'glue:UpdatePartition', 'glue:DeletePartition', 'glue:BatchDeletePartition', 'glue:GetPartition', 'glue:GetPartitions', 'glue:BatchGetPartition', 'glue:CreateUserDefinedFunction', 'glue:UpdateUserDefinedFunction', 'glue:DeleteUserDefinedFunction', 'glue:GetUserDefinedFunction', 'glue:GetUserDefinedFunctions']}]}}, {'Role': {'Path': '/', 'RoleName': 'EMR_DefaultRole', 'RoleId': 'AROAZ7YWVG74WPFFPUHFC', 'Arn': 'arn:aws:iam::686705424377:role/EMR_DefaultRole', 'CreateDate': '2020-01-27T11:22:07Z', 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}, 'RolePolicy': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Resource': '*', 'Action': ['ec2:AuthorizeSecurityGroupEgress', 'ec2:AuthorizeSecurityGroupIngress', 'ec2:CancelSpotInstanceRequests', 'ec2:CreateNetworkInterface', 'ec2:CreateSecurityGroup', 'ec2:CreateTags', 'ec2:DeleteNetworkInterface', 'ec2:DeleteSecurityGroup', 'ec2:DeleteTags', 'ec2:DescribeAvailabilityZones', 'ec2:DescribeAccountAttributes', 'ec2:DescribeDhcpOptions', 'ec2:DescribeImages', 'ec2:DescribeInstanceStatus', 'ec2:DescribeInstances', 'ec2:DescribeKeyPairs', 'ec2:DescribeNetworkAcls', 'ec2:DescribeNetworkInterfaces', 'ec2:DescribePrefixLists', 'ec2:DescribeRouteTables', 'ec2:DescribeSecurityGroups', 'ec2:DescribeSpotInstanceRequests', 'ec2:DescribeSpotPriceHistory', 'ec2:DescribeSubnets', 'ec2:DescribeTags', 'ec2:DescribeVpcAttribute', 'ec2:DescribeVpcEndpoints', 'ec2:DescribeVpcEndpointServices', 'ec2:DescribeVpcs', 'ec2:DetachNetworkInterface', 'ec2:ModifyImageAttribute', 'ec2:ModifyInstanceAttribute', 'ec2:RequestSpotInstances', 'ec2:RevokeSecurityGroupEgress', 'ec2:RunInstances', 'ec2:TerminateInstances', 'ec2:DeleteVolume', 'ec2:DescribeVolumeStatus', 'ec2:DescribeVolumes', 'ec2:DetachVolume', 'iam:GetRole', 'iam:GetRolePolicy', 'iam:ListInstanceProfiles', 'iam:ListRolePolicies', 'iam:PassRole', 's3:CreateBucket', 's3:Get*', 's3:List*', 'sdb:BatchPutAttributes', 'sdb:Select', 'sqs:CreateQueue', 'sqs:Delete*', 'sqs:GetQueue*', 'sqs:PurgeQueue', 'sqs:ReceiveMessage', 'cloudwatch:PutMetricAlarm', 'cloudwatch:DescribeAlarms', 'cloudwatch:DeleteAlarms', 'application-autoscaling:RegisterScalableTarget', 'application-autoscaling:DeregisterScalableTarget', 'application-autoscaling:PutScalingPolicy', 'application-autoscaling:DeleteScalingPolicy', 'application-autoscaling:Describe*']}, {'Effect': 'Allow', 'Action': 'iam:CreateServiceLinkedRole', 'Resource': 'arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot*', 'Condition': {'StringLike': {'iam:AWSServiceName': 'spot.amazonaws.com'}}}]}}]\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): elasticmapreduce.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "master_sg_id = emrs.create_security_group(ec2, '{}SG'.format(CLUSTER_NAME),\n",
    "    'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = emrs.create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME),\n",
    "    'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "\n",
    "keypair = emrs.recreate_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))\n",
    "\n",
    "emrs.recreate_default_roles(iam)\n",
    "\n",
    "cluster_id = emrs.create_emr_cluster(emr, CLUSTER_NAME,\n",
    "                master_sg_id,\n",
    "                slave_sg_id,\n",
    "                keypair['KeyName'], SUBNET_ID,\n",
    "                release_label='emr-5.28.1')\n",
    "cluster_dns = emrs.get_cluster_dns(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "#         .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#         .config(\"spark.eventLog.dir\" \"test_data/spark-logs\") \\\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXG\n"
     ]
    }
   ],
   "source": [
    "info_sdf = spark.read \\\n",
    "    .csv('s3a://short-interest-effect/data/raw/stock_info_nasdaq', header=True)\n",
    "symbol = info_sdf.limit(1).toPandas()['Symbol'][0]\n",
    "print(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sdf = spark.read \\\n",
    "    .csv('s3a://short-interest-effect/data/raw/short_interests_nasdaq', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, ShortExemptVolume: string, ShortVolume: string, SourceURL: string, Symbol: string, TotalVolume: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = short_sdf.select('Date').where(short_sdf.Symbol == F.lit(symbol)).orderBy(F.desc('Date')).limit(1) \\\n",
    "                .rdd.map(lambda r: r['Date']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = short_sdf.select('Date').where(short_sdf.Symbol == F.lit(symbol)) \\\n",
    "    .orderBy(F.desc('Date')).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates[0].Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Killed idle spark session id 8\n",
      "INFO:root:Sent spark session creation command to http://ec2-54-242-198-172.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Mon, 27 Jan 2020 13:42:32 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/9', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
      "INFO:root:{'id': 9, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Mon, 27 Jan 2020 13:42:32 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/9', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Mon, 27 Jan 2020 13:43:01 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/9/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.06819338422391857, 'state': 'running'}\n",
      "INFO:root:Progress: 0.06819338422391857\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.7175572519083969, 'state': 'running'}\n",
      "INFO:root:Progress: 0.7175572519083969\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.9896449704142012, 'state': 'running'}\n",
      "INFO:root:Progress: 0.9896449704142012\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': \"[Row(Date=u'2020-01-24')]\"},\n",
      "            'execution_count': 0,\n",
      "            'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/27 13:42:35 INFO RpcServer: Connected to the port 10003\n",
      "20/01/27 13:42:35 WARN RSCConf: Your hostname, ip-172-16-2-73.ec2.internal, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "20/01/27 13:42:35 WARN RSCConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "20/01/27 13:42:35 INFO RSCDriver: Received job request f06f6435-3990-462f-858b-4cfa59afaf89\n",
      "20/01/27 13:42:35 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "20/01/27 13:42:36 INFO SparkContext: Running Spark version 2.2.0\n",
      "20/01/27 13:42:37 INFO SparkContext: Submitted application: livy-session-9\n",
      "20/01/27 13:42:37 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/27 13:42:37 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/27 13:42:37 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/27 13:42:37 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/27 13:42:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/27 13:42:37 INFO Utils: Successfully started service 'sparkDriver' on port 34819.\n",
      "20/01/27 13:42:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/01/27 13:42:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/01/27 13:42:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/01/27 13:42:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/01/27 13:42:37 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-2f188539-b8e7-41a9-8974-e698113cc940\n",
      "20/01/27 13:42:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/01/27 13:42:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/01/27 13:42:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/01/27 13:42:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-16-2-73.ec2.internal:4040\n",
      "20/01/27 13:42:38 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/27 13:42:39 INFO RMProxy: Connecting to ResourceManager at ip-172-16-2-73.ec2.internal/172.16.2.73:8032\n",
      "20/01/27 13:42:39 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "20/01/27 13:42:39 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "20/01/27 13:42:39 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/01/27 13:42:39 INFO Client: Setting up container launch context for our AM\n",
      "20/01/27 13:42:39 INFO Client: Setting up the launch environment for our AM container\n",
      "20/01/27 13:42:39 INFO Client: Preparing resources for our AM container\n",
      "20/01/27 13:42:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/27 13:42:45 INFO Client: Uploading resource file:/mnt/tmp/spark-ef9a0fcd-b377-4d95-b176-b1500a0b8868/__spark_libs__5731267796806614030.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/__spark_libs__5731267796806614030.zip\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.0.29.Final.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/netty-all-4.0.29.Final.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-api-0.4.0-incubating.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-rsc-0.4.0-incubating.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-repl_2.11-0.4.0-incubating.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/commons-codec-1.9.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-core_2.11-0.4.0-incubating.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/com.epam_parso-2.0.8.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/27 13:42:47 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/hive-site.xml\n",
      "20/01/27 13:42:48 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/pyspark.zip\n",
      "20/01/27 13:42:48 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/py4j-0.10.4-src.zip\n",
      "20/01/27 13:42:48 WARN Client: Same path resource file:/usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache.\n",
      "20/01/27 13:42:48 WARN Client: Same path resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip added multiple times to distributed cache.\n",
      "20/01/27 13:42:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ef9a0fcd-b377-4d95-b176-b1500a0b8868/__spark_conf__5952328288445324768.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/__spark_conf__.zip\n",
      "20/01/27 13:42:48 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/27 13:42:48 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/27 13:42:48 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/27 13:42:48 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/27 13:42:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/27 13:42:48 INFO Client: Submitting application application_1580123518623_0010 to ResourceManager\n",
      "20/01/27 13:42:48 INFO YarnClientImpl: Submitted application application_1580123518623_0010\n",
      "20/01/27 13:42:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580123518623_0010 and attemptId None\n",
      "20/01/27 13:42:49 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "20/01/27 13:42:49 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580132568223\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0010/\n",
      "\t user: livy\n",
      "20/01/27 13:42:50 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "20/01/27 13:42:51 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "20/01/27 13:42:52 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "20/01/27 13:42:53 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "20/01/27 13:42:54 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "20/01/27 13:42:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/27 13:42:54 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-73.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0010), /proxy/application_1580123518623_0010\n",
      "20/01/27 13:42:54 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "20/01/27 13:42:55 INFO Client: Application report for application_1580123518623_0010 (state: RUNNING)\n",
      "20/01/27 13:42:55 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.225\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1580132568223\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0010/\n",
      "\t user: livy\n",
      "20/01/27 13:42:55 INFO YarnClientSchedulerBackend: Application application_1580123518623_0010 has started running.\n",
      "20/01/27 13:42:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45697.\n",
      "20/01/27 13:42:55 INFO NettyBlockTransferService: Server created on 172.16.2.73:45697\n",
      "20/01/27 13:42:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/27 13:42:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "20/01/27 13:42:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.73:45697 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "20/01/27 13:42:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "20/01/27 13:42:55 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/27 13:42:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "20/01/27 13:42:55 INFO EventLoggingListener: Logging events to hdfs:///var/log/spark/apps/application_1580123518623_0010\n",
      "20/01/27 13:42:55 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/27 13:42:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/27 13:42:35 INFO RpcServer: Connected to the port 10003\n",
      "INFO:root:20/01/27 13:42:35 WARN RSCConf: Your hostname, ip-172-16-2-73.ec2.internal, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "INFO:root:20/01/27 13:42:35 WARN RSCConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "INFO:root:20/01/27 13:42:35 INFO RSCDriver: Received job request f06f6435-3990-462f-858b-4cfa59afaf89\n",
      "INFO:root:20/01/27 13:42:35 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "INFO:root:20/01/27 13:42:36 INFO SparkContext: Running Spark version 2.2.0\n",
      "INFO:root:20/01/27 13:42:37 INFO SparkContext: Submitted application: livy-session-9\n",
      "INFO:root:20/01/27 13:42:37 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/27 13:42:37 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/27 13:42:37 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/27 13:42:37 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/27 13:42:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/27 13:42:37 INFO Utils: Successfully started service 'sparkDriver' on port 34819.\n",
      "INFO:root:20/01/27 13:42:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "INFO:root:20/01/27 13:42:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "INFO:root:20/01/27 13:42:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "INFO:root:20/01/27 13:42:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "INFO:root:20/01/27 13:42:37 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-2f188539-b8e7-41a9-8974-e698113cc940\n",
      "INFO:root:20/01/27 13:42:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "INFO:root:20/01/27 13:42:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "INFO:root:20/01/27 13:42:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "INFO:root:20/01/27 13:42:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-16-2-73.ec2.internal:4040\n",
      "INFO:root:20/01/27 13:42:38 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/27 13:42:39 INFO RMProxy: Connecting to ResourceManager at ip-172-16-2-73.ec2.internal/172.16.2.73:8032\n",
      "INFO:root:20/01/27 13:42:39 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "INFO:root:20/01/27 13:42:39 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "INFO:root:20/01/27 13:42:39 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "INFO:root:20/01/27 13:42:39 INFO Client: Setting up container launch context for our AM\n",
      "INFO:root:20/01/27 13:42:39 INFO Client: Setting up the launch environment for our AM container\n",
      "INFO:root:20/01/27 13:42:39 INFO Client: Preparing resources for our AM container\n",
      "INFO:root:20/01/27 13:42:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/27 13:42:45 INFO Client: Uploading resource file:/mnt/tmp/spark-ef9a0fcd-b377-4d95-b176-b1500a0b8868/__spark_libs__5731267796806614030.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/__spark_libs__5731267796806614030.zip\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.0.29.Final.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/netty-all-4.0.29.Final.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-api-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-rsc-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-repl_2.11-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/commons-codec-1.9.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/livy-core_2.11-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/27 13:42:47 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/hive-site.xml\n",
      "INFO:root:20/01/27 13:42:48 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/pyspark.zip\n",
      "INFO:root:20/01/27 13:42:48 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/py4j-0.10.4-src.zip\n",
      "INFO:root:20/01/27 13:42:48 WARN Client: Same path resource file:/usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache.\n",
      "INFO:root:20/01/27 13:42:48 WARN Client: Same path resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip added multiple times to distributed cache.\n",
      "INFO:root:20/01/27 13:42:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ef9a0fcd-b377-4d95-b176-b1500a0b8868/__spark_conf__5952328288445324768.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0010/__spark_conf__.zip\n",
      "INFO:root:20/01/27 13:42:48 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/27 13:42:48 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/27 13:42:48 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/27 13:42:48 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/27 13:42:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/27 13:42:48 INFO Client: Submitting application application_1580123518623_0010 to ResourceManager\n",
      "INFO:root:20/01/27 13:42:48 INFO YarnClientImpl: Submitted application application_1580123518623_0010\n",
      "INFO:root:20/01/27 13:42:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580123518623_0010 and attemptId None\n",
      "INFO:root:20/01/27 13:42:49 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 13:42:49 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580132568223\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0010/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/27 13:42:50 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 13:42:51 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 13:42:52 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 13:42:53 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 13:42:54 INFO Client: Application report for application_1580123518623_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 13:42:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/27 13:42:54 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-73.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0010), /proxy/application_1580123518623_0010\n",
      "INFO:root:20/01/27 13:42:54 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "INFO:root:20/01/27 13:42:55 INFO Client: Application report for application_1580123518623_0010 (state: RUNNING)\n",
      "INFO:root:20/01/27 13:42:55 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.225\n",
      "INFO:root:\t ApplicationMaster RPC port: 0\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580132568223\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0010/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/27 13:42:55 INFO YarnClientSchedulerBackend: Application application_1580123518623_0010 has started running.\n",
      "INFO:root:20/01/27 13:42:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45697.\n",
      "INFO:root:20/01/27 13:42:55 INFO NettyBlockTransferService: Server created on 172.16.2.73:45697\n",
      "INFO:root:20/01/27 13:42:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/27 13:42:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "INFO:root:20/01/27 13:42:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.73:45697 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "INFO:root:20/01/27 13:42:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "INFO:root:20/01/27 13:42:55 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/27 13:42:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.73, 45697, None)\n",
      "INFO:root:20/01/27 13:42:55 INFO EventLoggingListener: Logging events to hdfs:///var/log/spark/apps/application_1580123518623_0010\n",
      "INFO:root:20/01/27 13:42:55 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/27 13:42:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "}\n",
    "\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'debugging/Date-field-problem.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': 's3a://short-interest-effect',\n",
    "    'TABLE_STOCK_INFO_NASDAQ': '/data/raw/stock_info_nasdaq',\n",
    "    'TABLE_STOCK_INFO_NYSE': '/data/raw/stock_info_nyse',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_stock_info.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_id)\n",
    "print(master_sg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_si = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "#     'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "#     'STOCKS': STOCKS,\n",
    "    'LIMIT': None,\n",
    "    'STOCKS': [],\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests.py',\n",
    "        args=args_si,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_p = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "    'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "    'STOCKS': STOCKS,\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "    'URL': \"http://app.quotemedia.com/quotetools/getHistoryDownload.csv?&webmasterId=501&startDay={sd}&startMonth={sm}&startYear={sy}&endDay={ed}&endMonth={em}&endYear={ey}&isRanged=true&symbol={sym}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices.py',\n",
    "        args=args_p,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_q = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'STOCKS': STOCKS,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "            'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices_quality.py',\n",
    "        args=args_q,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_c = {\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine.py',\n",
    "        args=args_c,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.delete_cluster(emr, cluster_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

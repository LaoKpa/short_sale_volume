{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console\n",
    "\n",
    "In this notebook, we debug our DAGs without running then through Apache Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): ec2.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('airflow/dags/lib')\n",
    "import emrspark_lib as emrs\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "if config['App']['STOCKS'] == '':\n",
    "    STOCKS = []\n",
    "else:\n",
    "    STOCKS = json.loads(config.get('App', 'STOCKS').replace(\"'\", '\"'))\n",
    "\n",
    "ec2, emr, iam = emrs.get_boto_clients(config['AWS']['REGION_NAME'], config=config)\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = emrs.get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = emrs.get_first_available_subnet(ec2, VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found Security Group: sg-0890690aa6b78a376 in vpc vpc-d4fb5aaf (us-east-1).\n",
      "INFO:root:Found Security Group: sg-0486ff31a3c23dbda in vpc vpc-d4fb5aaf (us-east-1).\n",
      "INFO:root:keypair ShortInterestEffectDL_pem created:\n",
      "{'KeyFingerprint': '8d:b7:78:8c:60:ed:39:04:85:0f:b9:98:55:e2:ce:10:00:29:b3:b9', 'KeyMaterial': '-----BEGIN RSA PRIVATE KEY-----\\nMIIEpQIBAAKCAQEA2q1gbAPWJkITA7ZSLlk3Pj3AzRJIgjqtvn3XSd8aPvVcOXUQ3Cb3cBAUlAxT\\nx6QBmyWhLE9fLQtzlwCNGZcIxmmTa9Y8YsGg9lLRhk5pVw0yVlTROCNRpcTWvemHZgNK2nHj4K9n\\nw23iWy+XL8nyOtcCARrbdKg7lipavZ328OmUvBWFFyw6hl1hx0X9xmTccrsBgQAxSFfUIjLnG22M\\nmUdQTHrEKPgtF4BfY5is6+z/Enu/0wGl1+84BrCMsm8sTYP/f2GFn7L0wYSqgG8xvc9V9nhHHTUy\\nCtsxGL4aIKqP2zFkVjgVYTnr6Kamomd2QM0TpYjMDm7PNRlMBaK+CQIDAQABAoIBAQC4/JHkzhoK\\nozqSmVlL/AzPik3fLOhJwnnX+3xz8Veyir1Jnc51bkFDe1btJsB40jTJmjuN0FcjM9zEkKLMUwgJ\\nUKZ1QvYyVLplpgm5I/vYuEEs5vNWGgqnwKwoW7U+hup7gHzVRYzxtbMWkFvFAYD4ZYo8hQvA09AU\\naB/65/ZONLpMIMFgtSvpyNX3sOHL+59WMqjrit0HzMpPRJFD5FtDlswVkOS2VUsGm4DOSLalH6uR\\ndlkG/ADRSVul0vEFQQYBs2D7IpbnJrQ9wmmdUwFy65hbwUv7+C6fScY+TuT2qCxYf1HsWzoM81BB\\nc7FmtobeaBocjNRPeEar+BIhHjABAoGBAPEjZwrhuIVTBtxiVfCQGuYsdMsJ8Tf6OWGo9/kcMh9y\\ncUWp/S5uQ2gTtu4/Qv4uWxX1deFl+IriPb3E2YRR3lApqCAqh3aOQ4Xy7WYhuFtQofRWwlOJDzG/\\nMi0pt+m0ltRSMmOmaHZJegR9kvYqcJ4EbMo9yHofgkYFjLsntoMpAoGBAOgnl3AUi12U6eqJLpXo\\nWwzft6AM34CR+Ik/T+jcL67epOlnnu6TWtls0QR9nRp4UEzsGrqbCmYvDWaI2MY3G9ptn5KTQ+wA\\n0q1dOcXP17h6HsZKaB5LgOaM2+TwoDejDuqYPFmpBu3jAqHMuvG7YbBRJEFnkkms9qEako8gC5/h\\nAoGBALSX6QITTpg7ODYxR1+k7SBZRxcQ2SEtO3BSlXRxk3BNy1t7FeyCE+WMbkI8CrZGjV0CukgW\\nkG+jRM4s8SLSFYc/y3RbpZw6q4NUDkuhEWsCFmjh2SOq0EBXiwKb7esPnq5g6PAMnsuH9+QJD9lg\\nq3sMoVVNtCkSFATQ/zbjZSMRAoGAKvjhJEL5RRpCbkJ0WBVJfUmP8NVRLbhXW/hXL1BKl0DpH7cp\\niRnHkEV/LyidQwAQRRzTGcWHGEEPZvJfxdj/k+AE0Jnn0bCZTrPZ9M6zLJeWSLgPK8RoGMTsEyWj\\nUokhQrVpPmN2P+RsYUzHMeWwgHWUk3K7dl3IzSthA5M386ECgYEA6IS3QptzGOjdNNP7InjTCGEM\\n8pm/K4jVl2U+Ajxj1J7n6VlQIg2bWtkmzkhkbpv7I93HpEF99kMkPJ9C6Z7JYB1ReRYMATI4/rxk\\nlvyfsTNPIPW6lEoe9jQBt0WknFqAvv/0uxswISQ0vs3WfBK9tVxAeRowAka5nc4CJxFp7Xk=\\n-----END RSA PRIVATE KEY-----', 'KeyName': 'ShortInterestEffectDL_pem', 'ResponseMetadata': {'RequestId': '7c788050-0c1a-4832-92e2-5396e4352415', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8', 'content-length': '2102', 'vary': 'accept-encoding', 'date': 'Mon, 27 Jan 2020 14:17:47 GMT', 'server': 'AmazonEC2'}, 'RetryAttempts': 0}}\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): iam.amazonaws.com\n",
      "INFO:root:Output of recreate_default_roles:\n",
      "[{'Role': {'Path': '/', 'RoleName': 'EMR_EC2_DefaultRole', 'RoleId': 'AROAZ7YWVG744AZTZRC2H', 'Arn': 'arn:aws:iam::686705424377:role/EMR_EC2_DefaultRole', 'CreateDate': '2020-01-27T14:17:54Z', 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}, 'RolePolicy': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Resource': '*', 'Action': ['cloudwatch:*', 'dynamodb:*', 'ec2:Describe*', 'elasticmapreduce:Describe*', 'elasticmapreduce:ListBootstrapActions', 'elasticmapreduce:ListClusters', 'elasticmapreduce:ListInstanceGroups', 'elasticmapreduce:ListInstances', 'elasticmapreduce:ListSteps', 'kinesis:CreateStream', 'kinesis:DeleteStream', 'kinesis:DescribeStream', 'kinesis:GetRecords', 'kinesis:GetShardIterator', 'kinesis:MergeShards', 'kinesis:PutRecord', 'kinesis:SplitShard', 'rds:Describe*', 's3:*', 'sdb:*', 'sns:*', 'sqs:*', 'glue:CreateDatabase', 'glue:UpdateDatabase', 'glue:DeleteDatabase', 'glue:GetDatabase', 'glue:GetDatabases', 'glue:CreateTable', 'glue:UpdateTable', 'glue:DeleteTable', 'glue:GetTable', 'glue:GetTables', 'glue:GetTableVersions', 'glue:CreatePartition', 'glue:BatchCreatePartition', 'glue:UpdatePartition', 'glue:DeletePartition', 'glue:BatchDeletePartition', 'glue:GetPartition', 'glue:GetPartitions', 'glue:BatchGetPartition', 'glue:CreateUserDefinedFunction', 'glue:UpdateUserDefinedFunction', 'glue:DeleteUserDefinedFunction', 'glue:GetUserDefinedFunction', 'glue:GetUserDefinedFunctions']}]}}, {'Role': {'Path': '/', 'RoleName': 'EMR_DefaultRole', 'RoleId': 'AROAZ7YWVG742APXCLJS6', 'Arn': 'arn:aws:iam::686705424377:role/EMR_DefaultRole', 'CreateDate': '2020-01-27T14:18:03Z', 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}, 'RolePolicy': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Resource': '*', 'Action': ['ec2:AuthorizeSecurityGroupEgress', 'ec2:AuthorizeSecurityGroupIngress', 'ec2:CancelSpotInstanceRequests', 'ec2:CreateNetworkInterface', 'ec2:CreateSecurityGroup', 'ec2:CreateTags', 'ec2:DeleteNetworkInterface', 'ec2:DeleteSecurityGroup', 'ec2:DeleteTags', 'ec2:DescribeAvailabilityZones', 'ec2:DescribeAccountAttributes', 'ec2:DescribeDhcpOptions', 'ec2:DescribeImages', 'ec2:DescribeInstanceStatus', 'ec2:DescribeInstances', 'ec2:DescribeKeyPairs', 'ec2:DescribeNetworkAcls', 'ec2:DescribeNetworkInterfaces', 'ec2:DescribePrefixLists', 'ec2:DescribeRouteTables', 'ec2:DescribeSecurityGroups', 'ec2:DescribeSpotInstanceRequests', 'ec2:DescribeSpotPriceHistory', 'ec2:DescribeSubnets', 'ec2:DescribeTags', 'ec2:DescribeVpcAttribute', 'ec2:DescribeVpcEndpoints', 'ec2:DescribeVpcEndpointServices', 'ec2:DescribeVpcs', 'ec2:DetachNetworkInterface', 'ec2:ModifyImageAttribute', 'ec2:ModifyInstanceAttribute', 'ec2:RequestSpotInstances', 'ec2:RevokeSecurityGroupEgress', 'ec2:RunInstances', 'ec2:TerminateInstances', 'ec2:DeleteVolume', 'ec2:DescribeVolumeStatus', 'ec2:DescribeVolumes', 'ec2:DetachVolume', 'iam:GetRole', 'iam:GetRolePolicy', 'iam:ListInstanceProfiles', 'iam:ListRolePolicies', 'iam:PassRole', 's3:CreateBucket', 's3:Get*', 's3:List*', 'sdb:BatchPutAttributes', 'sdb:Select', 'sqs:CreateQueue', 'sqs:Delete*', 'sqs:GetQueue*', 'sqs:PurgeQueue', 'sqs:ReceiveMessage', 'cloudwatch:PutMetricAlarm', 'cloudwatch:DescribeAlarms', 'cloudwatch:DeleteAlarms', 'application-autoscaling:RegisterScalableTarget', 'application-autoscaling:DeregisterScalableTarget', 'application-autoscaling:PutScalingPolicy', 'application-autoscaling:DeleteScalingPolicy', 'application-autoscaling:Describe*']}, {'Effect': 'Allow', 'Action': 'iam:CreateServiceLinkedRole', 'Resource': 'arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot*', 'Condition': {'StringLike': {'iam:AWSServiceName': 'spot.amazonaws.com'}}}]}}]\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): elasticmapreduce.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "master_sg_id = emrs.create_security_group(ec2, '{}SG'.format(CLUSTER_NAME),\n",
    "    'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = emrs.create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME),\n",
    "    'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "\n",
    "keypair = emrs.recreate_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))\n",
    "\n",
    "emrs.recreate_default_roles(iam)\n",
    "\n",
    "cluster_id = emrs.create_emr_cluster(emr, CLUSTER_NAME,\n",
    "                master_sg_id,\n",
    "                slave_sg_id,\n",
    "                keypair['KeyName'], SUBNET_ID,\n",
    "                release_label='emr-5.28.1')\n",
    "cluster_dns = emrs.get_cluster_dns(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "#         .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#         .config(\"spark.eventLog.dir\" \"test_data/spark-logs\") \\\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXG\n"
     ]
    }
   ],
   "source": [
    "info_sdf = spark.read \\\n",
    "    .csv('s3a://short-interest-effect/data/raw/stock_info_nasdaq', header=True)\n",
    "symbol = info_sdf.limit(1).toPandas()['Symbol'][0]\n",
    "print(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: s3a://short-interest-effect/data/raw/short_interests_nasdaq;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o404.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: s3a://short-interest-effect/data/raw/short_interests_nasdaq;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:618)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-fce3da19ecc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshort_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://short-interest-effect/data/raw/short_interests_nasdaq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: s3a://short-interest-effect/data/raw/short_interests_nasdaq;'"
     ]
    }
   ],
   "source": [
    "short_sdf = spark.read \\\n",
    "    .csv('s3a://short-interest-effect/data/raw/short_interests_nasdaq', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = short_sdf.select('Date').where(short_sdf.Symbol == F.lit(symbol)).orderBy(F.desc('Date')).limit(1) \\\n",
    "                .rdd.map(lambda r: r['Date']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = short_sdf.select('Date').where(short_sdf.Symbol == F.lit(symbol)) \\\n",
    "    .orderBy(F.desc('Date')).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates[0].Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_spark_session_by_id(cluster_dns, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Sent spark session creation command to http://ec2-54-242-198-172.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Mon, 27 Jan 2020 14:35:39 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/17', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
      "INFO:root:{'id': 17, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Mon, 27 Jan 2020 14:35:39 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/17', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Mon, 27 Jan 2020 14:36:08 GMT', 'Content-Type': 'application/json', 'Content-Encoding': 'gzip', 'Location': '/sessions/17/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.2.16.v20160414)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': 'Table exists? False'},\n",
      "            'execution_count': 0,\n",
      "            'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/27 14:35:42 INFO RpcServer: Connected to the port 10003\n",
      "20/01/27 14:35:42 WARN RSCConf: Your hostname, ip-172-16-2-73.ec2.internal, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "20/01/27 14:35:42 WARN RSCConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "20/01/27 14:35:43 INFO RSCDriver: Received job request 97d852e5-394b-46d9-b61d-6d06037e5d2f\n",
      "20/01/27 14:35:43 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "20/01/27 14:35:43 INFO SparkContext: Running Spark version 2.2.0\n",
      "20/01/27 14:35:44 INFO SparkContext: Submitted application: livy-session-17\n",
      "20/01/27 14:35:44 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/27 14:35:44 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/27 14:35:44 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/27 14:35:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/27 14:35:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/27 14:35:44 INFO Utils: Successfully started service 'sparkDriver' on port 43861.\n",
      "20/01/27 14:35:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/01/27 14:35:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/01/27 14:35:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/01/27 14:35:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/01/27 14:35:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-13b7222f-f271-4efd-bdd7-00c5d6970128\n",
      "20/01/27 14:35:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/01/27 14:35:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/01/27 14:35:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/01/27 14:35:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-16-2-73.ec2.internal:4040\n",
      "20/01/27 14:35:45 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/27 14:35:46 INFO RMProxy: Connecting to ResourceManager at ip-172-16-2-73.ec2.internal/172.16.2.73:8032\n",
      "20/01/27 14:35:46 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "20/01/27 14:35:46 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "20/01/27 14:35:46 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/01/27 14:35:46 INFO Client: Setting up container launch context for our AM\n",
      "20/01/27 14:35:46 INFO Client: Setting up the launch environment for our AM container\n",
      "20/01/27 14:35:46 INFO Client: Preparing resources for our AM container\n",
      "20/01/27 14:35:48 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/27 14:35:52 INFO Client: Uploading resource file:/mnt/tmp/spark-5f0cdaf4-7b2f-4597-8b7b-718460e93d2e/__spark_libs__889138692966331801.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/__spark_libs__889138692966331801.zip\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.0.29.Final.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/netty-all-4.0.29.Final.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-api-0.4.0-incubating.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-rsc-0.4.0-incubating.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-repl_2.11-0.4.0-incubating.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/commons-codec-1.9.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-core_2.11-0.4.0-incubating.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/27 14:35:54 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/com.epam_parso-2.0.8.jar\n",
      "20/01/27 14:35:55 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/27 14:35:55 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/27 14:35:55 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/27 14:35:55 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/hive-site.xml\n",
      "20/01/27 14:35:55 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/pyspark.zip\n",
      "20/01/27 14:35:55 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/py4j-0.10.4-src.zip\n",
      "20/01/27 14:35:56 WARN Client: Same path resource file:/usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache.\n",
      "20/01/27 14:35:56 WARN Client: Same path resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip added multiple times to distributed cache.\n",
      "20/01/27 14:35:56 INFO Client: Uploading resource file:/mnt/tmp/spark-5f0cdaf4-7b2f-4597-8b7b-718460e93d2e/__spark_conf__3607636666497047458.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/__spark_conf__.zip\n",
      "20/01/27 14:35:56 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/27 14:35:56 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/27 14:35:56 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/27 14:35:56 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/27 14:35:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/27 14:35:56 INFO Client: Submitting application application_1580123518623_0018 to ResourceManager\n",
      "20/01/27 14:35:56 INFO YarnClientImpl: Submitted application application_1580123518623_0018\n",
      "20/01/27 14:35:56 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580123518623_0018 and attemptId None\n",
      "20/01/27 14:35:57 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "20/01/27 14:35:57 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580135756237\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0018/\n",
      "\t user: livy\n",
      "20/01/27 14:35:58 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "20/01/27 14:35:59 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "20/01/27 14:36:00 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "20/01/27 14:36:01 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "20/01/27 14:36:02 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "20/01/27 14:36:02 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/27 14:36:02 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-73.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0018), /proxy/application_1580123518623_0018\n",
      "20/01/27 14:36:02 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "20/01/27 14:36:03 INFO Client: Application report for application_1580123518623_0018 (state: RUNNING)\n",
      "20/01/27 14:36:03 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.225\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1580135756237\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0018/\n",
      "\t user: livy\n",
      "20/01/27 14:36:03 INFO YarnClientSchedulerBackend: Application application_1580123518623_0018 has started running.\n",
      "20/01/27 14:36:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35925.\n",
      "20/01/27 14:36:03 INFO NettyBlockTransferService: Server created on 172.16.2.73:35925\n",
      "20/01/27 14:36:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/27 14:36:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "20/01/27 14:36:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.73:35925 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "20/01/27 14:36:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "20/01/27 14:36:03 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/27 14:36:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "20/01/27 14:36:03 INFO EventLoggingListener: Logging events to hdfs:///var/log/spark/apps/application_1580123518623_0018\n",
      "20/01/27 14:36:03 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/27 14:36:03 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/27 14:35:42 INFO RpcServer: Connected to the port 10003\n",
      "INFO:root:20/01/27 14:35:42 WARN RSCConf: Your hostname, ip-172-16-2-73.ec2.internal, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "INFO:root:20/01/27 14:35:42 WARN RSCConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "INFO:root:20/01/27 14:35:43 INFO RSCDriver: Received job request 97d852e5-394b-46d9-b61d-6d06037e5d2f\n",
      "INFO:root:20/01/27 14:35:43 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "INFO:root:20/01/27 14:35:43 INFO SparkContext: Running Spark version 2.2.0\n",
      "INFO:root:20/01/27 14:35:44 INFO SparkContext: Submitted application: livy-session-17\n",
      "INFO:root:20/01/27 14:35:44 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/27 14:35:44 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/27 14:35:44 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/27 14:35:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/27 14:35:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/27 14:35:44 INFO Utils: Successfully started service 'sparkDriver' on port 43861.\n",
      "INFO:root:20/01/27 14:35:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "INFO:root:20/01/27 14:35:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "INFO:root:20/01/27 14:35:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "INFO:root:20/01/27 14:35:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "INFO:root:20/01/27 14:35:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-13b7222f-f271-4efd-bdd7-00c5d6970128\n",
      "INFO:root:20/01/27 14:35:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "INFO:root:20/01/27 14:35:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "INFO:root:20/01/27 14:35:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "INFO:root:20/01/27 14:35:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-16-2-73.ec2.internal:4040\n",
      "INFO:root:20/01/27 14:35:45 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/27 14:35:46 INFO RMProxy: Connecting to ResourceManager at ip-172-16-2-73.ec2.internal/172.16.2.73:8032\n",
      "INFO:root:20/01/27 14:35:46 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "INFO:root:20/01/27 14:35:46 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "INFO:root:20/01/27 14:35:46 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "INFO:root:20/01/27 14:35:46 INFO Client: Setting up container launch context for our AM\n",
      "INFO:root:20/01/27 14:35:46 INFO Client: Setting up the launch environment for our AM container\n",
      "INFO:root:20/01/27 14:35:46 INFO Client: Preparing resources for our AM container\n",
      "INFO:root:20/01/27 14:35:48 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/27 14:35:52 INFO Client: Uploading resource file:/mnt/tmp/spark-5f0cdaf4-7b2f-4597-8b7b-718460e93d2e/__spark_libs__889138692966331801.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/__spark_libs__889138692966331801.zip\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.0.29.Final.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/netty-all-4.0.29.Final.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-api-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-rsc-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-repl_2.11-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/commons-codec-1.9.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.4.0-incubating.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/livy-core_2.11-0.4.0-incubating.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/27 14:35:54 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/27 14:35:55 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/27 14:35:55 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/27 14:35:55 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/27 14:35:55 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/hive-site.xml\n",
      "INFO:root:20/01/27 14:35:55 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/pyspark.zip\n",
      "INFO:root:20/01/27 14:35:55 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/py4j-0.10.4-src.zip\n",
      "INFO:root:20/01/27 14:35:56 WARN Client: Same path resource file:/usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache.\n",
      "INFO:root:20/01/27 14:35:56 WARN Client: Same path resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip added multiple times to distributed cache.\n",
      "INFO:root:20/01/27 14:35:56 INFO Client: Uploading resource file:/mnt/tmp/spark-5f0cdaf4-7b2f-4597-8b7b-718460e93d2e/__spark_conf__3607636666497047458.zip -> hdfs://ip-172-16-2-73.ec2.internal:8020/user/livy/.sparkStaging/application_1580123518623_0018/__spark_conf__.zip\n",
      "INFO:root:20/01/27 14:35:56 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/27 14:35:56 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/27 14:35:56 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/27 14:35:56 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/27 14:35:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/27 14:35:56 INFO Client: Submitting application application_1580123518623_0018 to ResourceManager\n",
      "INFO:root:20/01/27 14:35:56 INFO YarnClientImpl: Submitted application application_1580123518623_0018\n",
      "INFO:root:20/01/27 14:35:56 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580123518623_0018 and attemptId None\n",
      "INFO:root:20/01/27 14:35:57 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 14:35:57 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580135756237\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0018/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/27 14:35:58 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 14:35:59 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 14:36:00 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 14:36:01 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 14:36:02 INFO Client: Application report for application_1580123518623_0018 (state: ACCEPTED)\n",
      "INFO:root:20/01/27 14:36:02 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/27 14:36:02 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-73.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0018), /proxy/application_1580123518623_0018\n",
      "INFO:root:20/01/27 14:36:02 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "INFO:root:20/01/27 14:36:03 INFO Client: Application report for application_1580123518623_0018 (state: RUNNING)\n",
      "INFO:root:20/01/27 14:36:03 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.225\n",
      "INFO:root:\t ApplicationMaster RPC port: 0\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580135756237\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-73.ec2.internal:20888/proxy/application_1580123518623_0018/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/27 14:36:03 INFO YarnClientSchedulerBackend: Application application_1580123518623_0018 has started running.\n",
      "INFO:root:20/01/27 14:36:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35925.\n",
      "INFO:root:20/01/27 14:36:03 INFO NettyBlockTransferService: Server created on 172.16.2.73:35925\n",
      "INFO:root:20/01/27 14:36:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/27 14:36:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "INFO:root:20/01/27 14:36:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.2.73:35925 with 366.3 MB RAM, BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "INFO:root:20/01/27 14:36:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "INFO:root:20/01/27 14:36:03 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/27 14:36:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.2.73, 35925, None)\n",
      "INFO:root:20/01/27 14:36:03 INFO EventLoggingListener: Logging events to hdfs:///var/log/spark/apps/application_1580123518623_0018\n",
      "INFO:root:20/01/27 14:36:03 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/27 14:36:03 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "}\n",
    "\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'debugging/spark_table_exists.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a8588718cb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0memrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill_all_inactive_spark_sessions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_dns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msession_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_spark_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_dns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0memrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_dns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/jay/DATA/learn/udacity-dend/short_interest_effect/airflow/dags/lib/emrspark_lib.py\u001b[0m in \u001b[0;36mkill_all_inactive_spark_sessions\u001b[0;34m(master_dns)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkill_all_inactive_spark_sessions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_dns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_dns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/sessions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0mspark_sessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sessions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Killing all inactive spark sessions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    601\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1251\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 160\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/envs/default/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "}\n",
    "\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'debugging/Date-field-problem.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': 's3a://short-interest-effect',\n",
    "    'TABLE_STOCK_INFO_NASDAQ': '/data/raw/stock_info_nasdaq',\n",
    "    'TABLE_STOCK_INFO_NYSE': '/data/raw/stock_info_nyse',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_stock_info.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_id)\n",
    "print(master_sg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_si = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "#     'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "#     'STOCKS': STOCKS,\n",
    "    'LIMIT': None,\n",
    "    'STOCKS': [],\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests.py',\n",
    "        args=args_si,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_p = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "    'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "    'STOCKS': STOCKS,\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "    'URL': \"http://app.quotemedia.com/quotetools/getHistoryDownload.csv?&webmasterId=501&startDay={sd}&startMonth={sm}&startYear={sy}&endDay={ed}&endMonth={em}&endYear={ey}&isRanged=true&symbol={sym}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices.py',\n",
    "        args=args_p,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_q = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'STOCKS': STOCKS,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "            'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices_quality.py',\n",
    "        args=args_q,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_c = {\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine.py',\n",
    "        args=args_c,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.delete_cluster(emr, cluster_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

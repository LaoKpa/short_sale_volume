{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console\n",
    "\n",
    "In this notebook, we debug our DAGs without running then through Apache Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('airflow/dags/lib')\n",
    "import emrspark_lib as emrs\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "if config['App']['STOCKS'] == '':\n",
    "    STOCKS = []\n",
    "else:\n",
    "    STOCKS = json.loads(config.get('App', 'STOCKS').replace(\"'\", '\"'))\n",
    "\n",
    "ec2, emr, iam = emrs.get_boto_clients(config['AWS']['REGION_NAME'], config=config)\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = emrs.get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = emrs.get_first_available_subnet(ec2, VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found Security Group: sg-0d32ef88aafc53667 in vpc vpc-d4fb5aaf (us-east-1).\n",
      "INFO:root:Found Security Group: sg-0e715d65ff15edd5d in vpc vpc-d4fb5aaf (us-east-1).\n",
      "INFO:root:keypair ShortInterestEffectDL_pem created:\n",
      "{'KeyFingerprint': 'f7:9c:a0:7a:fb:fb:44:43:51:cc:82:7e:dd:e3:11:90:12:f3:f7:f5', 'KeyMaterial': '-----BEGIN RSA PRIVATE KEY-----\\nMIIEowIBAAKCAQEArwftCKZcfsfB679dWvEw7WKhqHg8ZG6TnbjWfrqlQuugq6wvF3NiDc51DkHp\\n5p7yc04FmBPY1YXRyRZkuRaeMtLsuzbh1O+UVHeT2imtuRIU8fVg0VtTlmmi7mK0Vu9CIZOyG+VB\\nnbcaoxbh9PkUBwBKIHExUSSRYSaROVHsqnPNRqHuxDgkwNGyqBPMOsddaSFOyqEdKOZkXxTwQicx\\nP8dgaEDjz+nzhCDb2Aa/oPpBFlwbegULZWqgXIrYyAwGVGg5PNBUHciGq6kXNQfbfDjVTkfzL1mA\\nu2wZ3irhOFLzOqiqM/fDXnKCr7hyM799PpkwCSUSiuw+MxJFhAe3cQIDAQABAoIBAFc9MxbBnI6Q\\nUAaJziUa3FElfP+0Vh2Uw7y0OSzXKLHPMbj/TEV6/B93jP57OfL5vExeUgl3svbSCTsaDz/1lwor\\n+VRiyXLRqvPi2e2+IlmBOrFSpVwUEfiBVHVO+yXtgN+tdvPSc78BWaR6LktkrmfC7C95W6Re/psW\\nlwGj5Lsbw+rbJdPBziNCVU384uegnTSzvGdyYXQcLcE4n/qN6YT11PMeTX0N4a8qHQyqIcxUPz7d\\nyTf2YfVbylfxDip88RcjCpAmRkJK3G6eWc7nuOs3mQcdxmanU/qKNS5Wacf+BMUgBjK1yJ9SAIMa\\nkg2AOSuXX6u8XYxqG8VUZSy/SPECgYEA1VgkZMDGqObBJTKfRTgIKyE3tEjVWULNKWLYbc8Lh7MZ\\na9arhgwCqxram8YUsFzNA3DQdiGZKJEXEnKEmMyA9vzlox3V5Cjt8nnk6w46SKElMqoEPpFBuEFW\\nTGFoxZJpo2SL5ce/fT85ckJj/uq/ef6+8omUz+bVZtebcojEV5UCgYEA0ga+7UN/w42iG3UT12xt\\nMqGP3BQyAF3GRH8XC+dbbLX6fFTuDerv8+kvlWcOLvjEeDBtVsm12ICIoTgYRkWEXs1LGhLG8bQg\\nQM+1s6dT71Obl43qHX2gMvwOA8da8mXoNWySQFBx9eBxTAMQxoPUzSKtrR0pAPuyBdwB7eileW0C\\ngYA3OgLpxfFnJZzYCTurim6q44Vs+PGpkp5TWVAXVOJyNU2eWcq6/37tVh1C98rBQLSdcGW2PspL\\n8wsa7mQevcmgpiS3zfBhiKrX+u/kXSF/TZ+4dsR0gduWW+uACDM7+cO41T1VtRh/zo8LYd5Kze/V\\nH7nERRMtol6ZqbSGuJrC5QKBgGwiCP8Dd65pJ4NXfgW1z2r3/6dhxnSl00TE4krArTOFZhfJal9E\\nDJLaBiO30XehoRX4aPL8G2ICm1XWGblkZUiiDf64mDEtekWkK3r5tZ9MvpkC+fyBB4/4cgbEBzEy\\nAqIu6gWu5V2gVvS5x1NN4KxVJHIyxz+4VunpYhzpYPjpAoGBAIFBqw2aq8ldSvhtwTKHwKrNA55a\\n2PGulomhEpEX1EfV3H3s2bQsz8gq9OwXyvpcK0cjz/iplN3XOY5bYaCzudQ2W93L4v1wdJd79eyD\\nXbYh1HQd88EfO5ChQjFRQptnfUwsnzr0YnRlhHcLvNh498Lw16e4R0y7lBBgHaHIuT4v\\n-----END RSA PRIVATE KEY-----', 'KeyName': 'ShortInterestEffectDL_pem', 'ResponseMetadata': {'RequestId': '0627be8b-8942-4a90-9f81-1ebf99b6bf4e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8', 'content-length': '2098', 'vary': 'accept-encoding', 'date': 'Sun, 26 Jan 2020 21:27:50 GMT', 'server': 'AmazonEC2'}, 'RetryAttempts': 0}}\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): iam.amazonaws.com\n",
      "INFO:root:Output of recreate_default_roles:\n",
      "[{'Role': {'Path': '/', 'RoleName': 'EMR_EC2_DefaultRole', 'RoleId': 'AROAZ7YWVG745NMKGAYXR', 'Arn': 'arn:aws:iam::686705424377:role/EMR_EC2_DefaultRole', 'CreateDate': '2020-01-26T21:27:55Z', 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}, 'RolePolicy': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Resource': '*', 'Action': ['cloudwatch:*', 'dynamodb:*', 'ec2:Describe*', 'elasticmapreduce:Describe*', 'elasticmapreduce:ListBootstrapActions', 'elasticmapreduce:ListClusters', 'elasticmapreduce:ListInstanceGroups', 'elasticmapreduce:ListInstances', 'elasticmapreduce:ListSteps', 'kinesis:CreateStream', 'kinesis:DeleteStream', 'kinesis:DescribeStream', 'kinesis:GetRecords', 'kinesis:GetShardIterator', 'kinesis:MergeShards', 'kinesis:PutRecord', 'kinesis:SplitShard', 'rds:Describe*', 's3:*', 'sdb:*', 'sns:*', 'sqs:*', 'glue:CreateDatabase', 'glue:UpdateDatabase', 'glue:DeleteDatabase', 'glue:GetDatabase', 'glue:GetDatabases', 'glue:CreateTable', 'glue:UpdateTable', 'glue:DeleteTable', 'glue:GetTable', 'glue:GetTables', 'glue:GetTableVersions', 'glue:CreatePartition', 'glue:BatchCreatePartition', 'glue:UpdatePartition', 'glue:DeletePartition', 'glue:BatchDeletePartition', 'glue:GetPartition', 'glue:GetPartitions', 'glue:BatchGetPartition', 'glue:CreateUserDefinedFunction', 'glue:UpdateUserDefinedFunction', 'glue:DeleteUserDefinedFunction', 'glue:GetUserDefinedFunction', 'glue:GetUserDefinedFunctions']}]}}, {'Role': {'Path': '/', 'RoleName': 'EMR_DefaultRole', 'RoleId': 'AROAZ7YWVG744OVMAJPJS', 'Arn': 'arn:aws:iam::686705424377:role/EMR_DefaultRole', 'CreateDate': '2020-01-26T21:28:04Z', 'AssumeRolePolicyDocument': {'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}}, 'RolePolicy': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Resource': '*', 'Action': ['ec2:AuthorizeSecurityGroupEgress', 'ec2:AuthorizeSecurityGroupIngress', 'ec2:CancelSpotInstanceRequests', 'ec2:CreateNetworkInterface', 'ec2:CreateSecurityGroup', 'ec2:CreateTags', 'ec2:DeleteNetworkInterface', 'ec2:DeleteSecurityGroup', 'ec2:DeleteTags', 'ec2:DescribeAvailabilityZones', 'ec2:DescribeAccountAttributes', 'ec2:DescribeDhcpOptions', 'ec2:DescribeImages', 'ec2:DescribeInstanceStatus', 'ec2:DescribeInstances', 'ec2:DescribeKeyPairs', 'ec2:DescribeNetworkAcls', 'ec2:DescribeNetworkInterfaces', 'ec2:DescribePrefixLists', 'ec2:DescribeRouteTables', 'ec2:DescribeSecurityGroups', 'ec2:DescribeSpotInstanceRequests', 'ec2:DescribeSpotPriceHistory', 'ec2:DescribeSubnets', 'ec2:DescribeTags', 'ec2:DescribeVpcAttribute', 'ec2:DescribeVpcEndpoints', 'ec2:DescribeVpcEndpointServices', 'ec2:DescribeVpcs', 'ec2:DetachNetworkInterface', 'ec2:ModifyImageAttribute', 'ec2:ModifyInstanceAttribute', 'ec2:RequestSpotInstances', 'ec2:RevokeSecurityGroupEgress', 'ec2:RunInstances', 'ec2:TerminateInstances', 'ec2:DeleteVolume', 'ec2:DescribeVolumeStatus', 'ec2:DescribeVolumes', 'ec2:DetachVolume', 'iam:GetRole', 'iam:GetRolePolicy', 'iam:ListInstanceProfiles', 'iam:ListRolePolicies', 'iam:PassRole', 's3:CreateBucket', 's3:Get*', 's3:List*', 'sdb:BatchPutAttributes', 'sdb:Select', 'sqs:CreateQueue', 'sqs:Delete*', 'sqs:GetQueue*', 'sqs:PurgeQueue', 'sqs:ReceiveMessage', 'cloudwatch:PutMetricAlarm', 'cloudwatch:DescribeAlarms', 'cloudwatch:DeleteAlarms', 'application-autoscaling:RegisterScalableTarget', 'application-autoscaling:DeregisterScalableTarget', 'application-autoscaling:PutScalingPolicy', 'application-autoscaling:DeleteScalingPolicy', 'application-autoscaling:Describe*']}, {'Effect': 'Allow', 'Action': 'iam:CreateServiceLinkedRole', 'Resource': 'arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot*', 'Condition': {'StringLike': {'iam:AWSServiceName': 'spot.amazonaws.com'}}}]}}]\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): elasticmapreduce.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "master_sg_id = emrs.create_security_group(ec2, '{}SG'.format(CLUSTER_NAME),\n",
    "    'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = emrs.create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME),\n",
    "    'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "\n",
    "keypair = emrs.recreate_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))\n",
    "\n",
    "emrs.recreate_default_roles(iam)\n",
    "\n",
    "cluster_id = emrs.create_emr_cluster(emr, CLUSTER_NAME,\n",
    "                master_sg_id,\n",
    "                slave_sg_id,\n",
    "                keypair['KeyName'], SUBNET_ID,\n",
    "                release_label='emr-5.28.1')\n",
    "cluster_dns = emrs.get_cluster_dns(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': 's3a://short-interest-effect',\n",
    "    'TABLE_STOCK_INFO_NASDAQ': '/data/raw/stock_info_nasdaq',\n",
    "    'TABLE_STOCK_INFO_NYSE': '/data/raw/stock_info_nyse',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Killed idle spark session id 4\n",
      "INFO:root:Sent spark session creation command to http://ec2-100-26-146-164.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Sun, 26 Jan 2020 21:32:47 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/5', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:{'id': 5, 'name': None, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Sun, 26 Jan 2020 21:32:47 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/5', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Sun, 26 Jan 2020 21:33:27 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/5/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: waiting\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.25, 'state': 'running'}\n",
      "INFO:root:Progress: 0.25\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': ''}, 'execution_count': 0, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/26 21:32:58 INFO Client: Setting up the launch environment for our AM container\n",
      "20/01/26 21:32:58 INFO Client: Preparing resources for our AM container\n",
      "20/01/26 21:32:58 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/26 21:33:01 INFO Client: Uploading resource file:/mnt/tmp/spark-b9f542e4-e0fa-464d-96a2-7758a8b66581/__spark_libs__7104217233315212332.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/__spark_libs__7104217233315212332.zip\n",
      "20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-api-0.6.0-incubating.jar\n",
      "20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/netty-all-4.1.17.Final.jar\n",
      "20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-rsc-0.6.0-incubating.jar\n",
      "20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/commons-codec-1.9.jar\n",
      "20/01/26 21:33:03 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-core_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:33:03 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-repl_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:33:03 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/26 21:33:04 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/com.epam_parso-2.0.8.jar\n",
      "20/01/26 21:33:04 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/26 21:33:05 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/26 21:33:05 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/26 21:33:05 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/hive-site.xml\n",
      "20/01/26 21:33:06 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/sparkr.zip\n",
      "20/01/26 21:33:06 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/pyspark.zip\n",
      "20/01/26 21:33:06 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/py4j-0.10.7-src.zip\n",
      "20/01/26 21:33:07 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/01/26 21:33:07 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:33:07 INFO Client: Uploading resource file:/mnt/tmp/spark-b9f542e4-e0fa-464d-96a2-7758a8b66581/__spark_conf__8058689461207267163.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/__spark_conf__.zip\n",
      "20/01/26 21:33:07 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/26 21:33:07 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/26 21:33:07 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/26 21:33:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/26 21:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/26 21:33:09 INFO Client: Submitting application application_1580072780075_0006 to ResourceManager\n",
      "20/01/26 21:33:09 INFO YarnClientImpl: Submitted application application_1580072780075_0006\n",
      "20/01/26 21:33:09 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0006 and attemptId None\n",
      "20/01/26 21:33:10 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "20/01/26 21:33:10 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074389265\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0006/\n",
      "\t user: livy\n",
      "20/01/26 21:33:11 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "20/01/26 21:33:12 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "20/01/26 21:33:13 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "20/01/26 21:33:14 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "20/01/26 21:33:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0006), /proxy/application_1580072780075_0006\n",
      "20/01/26 21:33:15 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/01/26 21:33:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/26 21:33:15 INFO Client: Application report for application_1580072780075_0006 (state: RUNNING)\n",
      "20/01/26 21:33:15 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.114\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074389265\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0006/\n",
      "\t user: livy\n",
      "20/01/26 21:33:15 INFO YarnClientSchedulerBackend: Application application_1580072780075_0006 has started running.\n",
      "20/01/26 21:33:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45731.\n",
      "20/01/26 21:33:15 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:45731\n",
      "20/01/26 21:33:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/26 21:33:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "20/01/26 21:33:15 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:45731 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "20/01/26 21:33:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "20/01/26 21:33:15 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/26 21:33:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "20/01/26 21:33:15 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/01/26 21:33:15 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0006\n",
      "20/01/26 21:33:15 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/26 21:33:15 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/01/26 21:33:16 INFO SparkEntries: Spark context finished initialization in 19774ms\n",
      "20/01/26 21:33:16 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "20/01/26 21:33:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:39544) with ID 3\n",
      "20/01/26 21:33:19 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 1)\n",
      "20/01/26 21:33:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:45535 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-114.ec2.internal, 45535, None)\n",
      "20/01/26 21:33:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:56390) with ID 2\n",
      "20/01/26 21:33:22 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)\n",
      "20/01/26 21:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:56392) with ID 1\n",
      "20/01/26 21:33:23 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 3)\n",
      "20/01/26 21:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:56572) with ID 4\n",
      "20/01/26 21:33:23 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 4)\n",
      "20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:42245 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-244.ec2.internal, 42245, None)\n",
      "20/01/26 21:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:56574) with ID 5\n",
      "20/01/26 21:33:23 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 5)\n",
      "20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:45385 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-35.ec2.internal, 45385, None)\n",
      "20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:43525 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-244.ec2.internal, 43525, None)\n",
      "20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:34913 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-35.ec2.internal, 34913, None)\n",
      "20/01/26 21:33:28 INFO SparkEntries: Created HiveContext.\n",
      "20/01/26 21:33:33 WARN TaskSetManager: Stage 0 contains a task of very large size (478 KB). The maximum recommended task size is 100 KB.\n",
      "20/01/26 21:33:50 WARN DAG: Stored data from https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download to s3a://short-interest-effect/data/raw/stock_info_nasdaq.\n",
      "20/01/26 21:33:51 WARN TaskSetManager: Stage 1 contains a task of very large size (396 KB). The maximum recommended task size is 100 KB.\n",
      "20/01/26 21:34:05 WARN DAG: Stored data from https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download to s3a://short-interest-effect/data/raw/stock_info_nyse.\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/26 21:32:58 INFO Client: Setting up the launch environment for our AM container\n",
      "INFO:root:20/01/26 21:32:58 INFO Client: Preparing resources for our AM container\n",
      "INFO:root:20/01/26 21:32:58 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/26 21:33:01 INFO Client: Uploading resource file:/mnt/tmp/spark-b9f542e4-e0fa-464d-96a2-7758a8b66581/__spark_libs__7104217233315212332.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/__spark_libs__7104217233315212332.zip\n",
      "INFO:root:20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-api-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/netty-all-4.1.17.Final.jar\n",
      "INFO:root:20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-rsc-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:33:02 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/commons-codec-1.9.jar\n",
      "INFO:root:20/01/26 21:33:03 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-core_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:33:03 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/livy-repl_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:33:03 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/26 21:33:04 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/26 21:33:04 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/26 21:33:05 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/26 21:33:05 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/26 21:33:05 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/hive-site.xml\n",
      "INFO:root:20/01/26 21:33:06 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/sparkr.zip\n",
      "INFO:root:20/01/26 21:33:06 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/pyspark.zip\n",
      "INFO:root:20/01/26 21:33:06 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/py4j-0.10.7-src.zip\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:33:07 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:33:07 INFO Client: Uploading resource file:/mnt/tmp/spark-b9f542e4-e0fa-464d-96a2-7758a8b66581/__spark_conf__8058689461207267163.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0006/__spark_conf__.zip\n",
      "INFO:root:20/01/26 21:33:07 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/26 21:33:07 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/26 21:33:07 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/26 21:33:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/26 21:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/26 21:33:09 INFO Client: Submitting application application_1580072780075_0006 to ResourceManager\n",
      "INFO:root:20/01/26 21:33:09 INFO YarnClientImpl: Submitted application application_1580072780075_0006\n",
      "INFO:root:20/01/26 21:33:09 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0006 and attemptId None\n",
      "INFO:root:20/01/26 21:33:10 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:33:10 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074389265\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0006/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:33:11 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:33:12 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:33:13 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:33:14 INFO Client: Application report for application_1580072780075_0006 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:33:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0006), /proxy/application_1580072780075_0006\n",
      "INFO:root:20/01/26 21:33:15 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "INFO:root:20/01/26 21:33:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/26 21:33:15 INFO Client: Application report for application_1580072780075_0006 (state: RUNNING)\n",
      "INFO:root:20/01/26 21:33:15 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.114\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074389265\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0006/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:33:15 INFO YarnClientSchedulerBackend: Application application_1580072780075_0006 has started running.\n",
      "INFO:root:20/01/26 21:33:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45731.\n",
      "INFO:root:20/01/26 21:33:15 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:45731\n",
      "INFO:root:20/01/26 21:33:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/26 21:33:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "INFO:root:20/01/26 21:33:15 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:45731 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "INFO:root:20/01/26 21:33:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "INFO:root:20/01/26 21:33:15 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/26 21:33:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 45731, None)\n",
      "INFO:root:20/01/26 21:33:15 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "INFO:root:20/01/26 21:33:15 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0006\n",
      "INFO:root:20/01/26 21:33:15 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/26 21:33:15 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "INFO:root:20/01/26 21:33:16 INFO SparkEntries: Spark context finished initialization in 19774ms\n",
      "INFO:root:20/01/26 21:33:16 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "INFO:root:20/01/26 21:33:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:39544) with ID 3\n",
      "INFO:root:20/01/26 21:33:19 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 1)\n",
      "INFO:root:20/01/26 21:33:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:45535 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-114.ec2.internal, 45535, None)\n",
      "INFO:root:20/01/26 21:33:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:56390) with ID 2\n",
      "INFO:root:20/01/26 21:33:22 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)\n",
      "INFO:root:20/01/26 21:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:56392) with ID 1\n",
      "INFO:root:20/01/26 21:33:23 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 3)\n",
      "INFO:root:20/01/26 21:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:56572) with ID 4\n",
      "INFO:root:20/01/26 21:33:23 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 4)\n",
      "INFO:root:20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:42245 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-244.ec2.internal, 42245, None)\n",
      "INFO:root:20/01/26 21:33:23 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:56574) with ID 5\n",
      "INFO:root:20/01/26 21:33:23 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 5)\n",
      "INFO:root:20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:45385 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-35.ec2.internal, 45385, None)\n",
      "INFO:root:20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:43525 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-244.ec2.internal, 43525, None)\n",
      "INFO:root:20/01/26 21:33:23 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:34913 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-35.ec2.internal, 34913, None)\n",
      "INFO:root:20/01/26 21:33:28 INFO SparkEntries: Created HiveContext.\n",
      "INFO:root:20/01/26 21:33:33 WARN TaskSetManager: Stage 0 contains a task of very large size (478 KB). The maximum recommended task size is 100 KB.\n",
      "INFO:root:20/01/26 21:33:50 WARN DAG: Stored data from https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download to s3a://short-interest-effect/data/raw/stock_info_nasdaq.\n",
      "INFO:root:20/01/26 21:33:51 WARN TaskSetManager: Stage 1 contains a task of very large size (396 KB). The maximum recommended task size is 100 KB.\n",
      "INFO:root:20/01/26 21:34:05 WARN DAG: Stored data from https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download to s3a://short-interest-effect/data/raw/stock_info_nyse.\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_stock_info.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_si = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "    'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "    'STOCKS': STOCKS,\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Sent spark session creation command to http://ec2-100-26-146-164.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Sun, 26 Jan 2020 21:35:32 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/6', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:{'id': 6, 'name': None, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Sun, 26 Jan 2020 21:35:32 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/6', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Sun, 26 Jan 2020 21:36:11 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/6/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: waiting\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.2, 'state': 'running'}\n",
      "INFO:root:Progress: 0.2\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.5555555555555556, 'state': 'running'}\n",
      "INFO:root:Progress: 0.5555555555555556\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.952, 'state': 'running'}\n",
      "INFO:root:Progress: 0.952\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.9310344827586207, 'state': 'running'}\n",
      "INFO:root:Progress: 0.9310344827586207\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.8787878787878788, 'state': 'running'}\n",
      "INFO:root:Progress: 0.8787878787878788\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.9130434782608695, 'state': 'running'}\n",
      "INFO:root:Progress: 0.9130434782608695\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.92, 'state': 'running'}\n",
      "INFO:root:Progress: 0.92\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': '/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py:346: '\n",
      "                                   'UserWarning: inferring schema from dict is '\n",
      "                                   'deprecated,please use pyspark.sql.Row '\n",
      "                                   'instead'},\n",
      "            'execution_count': 0,\n",
      "            'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/26 21:35:42 INFO Client: Setting up the launch environment for our AM container\n",
      "20/01/26 21:35:42 INFO Client: Preparing resources for our AM container\n",
      "20/01/26 21:35:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/26 21:35:45 INFO Client: Uploading resource file:/mnt/tmp/spark-1c76aacb-7b65-4c62-8124-2ae398b17566/__spark_libs__8176697373775778793.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/__spark_libs__8176697373775778793.zip\n",
      "20/01/26 21:35:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-api-0.6.0-incubating.jar\n",
      "20/01/26 21:35:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/netty-all-4.1.17.Final.jar\n",
      "20/01/26 21:35:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-rsc-0.6.0-incubating.jar\n",
      "20/01/26 21:35:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/commons-codec-1.9.jar\n",
      "20/01/26 21:35:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-core_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:35:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-repl_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:35:48 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/26 21:35:48 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/com.epam_parso-2.0.8.jar\n",
      "20/01/26 21:35:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/26 21:35:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/26 21:35:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/26 21:35:50 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/hive-site.xml\n",
      "20/01/26 21:35:50 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/sparkr.zip\n",
      "20/01/26 21:35:50 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/pyspark.zip\n",
      "20/01/26 21:35:50 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/py4j-0.10.7-src.zip\n",
      "20/01/26 21:35:51 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/01/26 21:35:51 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:35:51 INFO Client: Uploading resource file:/mnt/tmp/spark-1c76aacb-7b65-4c62-8124-2ae398b17566/__spark_conf__3149509475309909625.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/__spark_conf__.zip\n",
      "20/01/26 21:35:51 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/26 21:35:51 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/26 21:35:51 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/26 21:35:51 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/26 21:35:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/26 21:35:53 INFO Client: Submitting application application_1580072780075_0007 to ResourceManager\n",
      "20/01/26 21:35:53 INFO YarnClientImpl: Submitted application application_1580072780075_0007\n",
      "20/01/26 21:35:53 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0007 and attemptId None\n",
      "20/01/26 21:35:54 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "20/01/26 21:35:54 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074553357\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0007/\n",
      "\t user: livy\n",
      "20/01/26 21:35:55 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "20/01/26 21:35:56 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "20/01/26 21:35:57 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "20/01/26 21:35:58 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "20/01/26 21:35:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0007), /proxy/application_1580072780075_0007\n",
      "20/01/26 21:35:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/01/26 21:35:59 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/26 21:35:59 INFO Client: Application report for application_1580072780075_0007 (state: RUNNING)\n",
      "20/01/26 21:35:59 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.114\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074553357\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0007/\n",
      "\t user: livy\n",
      "20/01/26 21:35:59 INFO YarnClientSchedulerBackend: Application application_1580072780075_0007 has started running.\n",
      "20/01/26 21:35:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35023.\n",
      "20/01/26 21:35:59 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:35023\n",
      "20/01/26 21:35:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/26 21:35:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "20/01/26 21:35:59 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:35023 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "20/01/26 21:35:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "20/01/26 21:35:59 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/26 21:35:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "20/01/26 21:35:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/01/26 21:36:00 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0007\n",
      "20/01/26 21:36:00 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/26 21:36:00 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/01/26 21:36:00 INFO SparkEntries: Spark context finished initialization in 19492ms\n",
      "20/01/26 21:36:00 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "20/01/26 21:36:03 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:40034) with ID 1\n",
      "20/01/26 21:36:03 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)\n",
      "20/01/26 21:36:04 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:33533 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-114.ec2.internal, 33533, None)\n",
      "20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:35374) with ID 3\n",
      "20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 2)\n",
      "20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:53966) with ID 5\n",
      "20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 3)\n",
      "20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:53968) with ID 4\n",
      "20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 4)\n",
      "20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:35376) with ID 2\n",
      "20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 5)\n",
      "20/01/26 21:36:08 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:41525 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-35.ec2.internal, 41525, None)\n",
      "20/01/26 21:36:08 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:42911 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-244.ec2.internal, 42911, None)\n",
      "20/01/26 21:36:08 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:44005 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-244.ec2.internal, 44005, None)\n",
      "20/01/26 21:36:09 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:32831 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-35.ec2.internal, 32831, None)\n",
      "20/01/26 21:36:12 INFO SparkEntries: Created HiveContext.\n",
      "20/01/26 21:36:21 WARN DAG: 0/7 - total rows in this batch: 1714\n",
      "20/01/26 21:37:45 WARN DAG: done!\n",
      "20/01/26 21:37:46 WARN DAG: 0/7 - total rows in this batch: 1713\n",
      "20/01/26 21:39:05 WARN DAG: done!\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/26 21:35:42 INFO Client: Setting up the launch environment for our AM container\n",
      "INFO:root:20/01/26 21:35:42 INFO Client: Preparing resources for our AM container\n",
      "INFO:root:20/01/26 21:35:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/26 21:35:45 INFO Client: Uploading resource file:/mnt/tmp/spark-1c76aacb-7b65-4c62-8124-2ae398b17566/__spark_libs__8176697373775778793.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/__spark_libs__8176697373775778793.zip\n",
      "INFO:root:20/01/26 21:35:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-api-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:35:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/netty-all-4.1.17.Final.jar\n",
      "INFO:root:20/01/26 21:35:46 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-rsc-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:35:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/commons-codec-1.9.jar\n",
      "INFO:root:20/01/26 21:35:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-core_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:35:47 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/livy-repl_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:35:48 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/26 21:35:48 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/26 21:35:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/26 21:35:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/26 21:35:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/26 21:35:50 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/hive-site.xml\n",
      "INFO:root:20/01/26 21:35:50 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/sparkr.zip\n",
      "INFO:root:20/01/26 21:35:50 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/pyspark.zip\n",
      "INFO:root:20/01/26 21:35:50 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/py4j-0.10.7-src.zip\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:35:51 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:35:51 INFO Client: Uploading resource file:/mnt/tmp/spark-1c76aacb-7b65-4c62-8124-2ae398b17566/__spark_conf__3149509475309909625.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0007/__spark_conf__.zip\n",
      "INFO:root:20/01/26 21:35:51 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/26 21:35:51 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/26 21:35:51 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/26 21:35:51 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/26 21:35:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/26 21:35:53 INFO Client: Submitting application application_1580072780075_0007 to ResourceManager\n",
      "INFO:root:20/01/26 21:35:53 INFO YarnClientImpl: Submitted application application_1580072780075_0007\n",
      "INFO:root:20/01/26 21:35:53 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0007 and attemptId None\n",
      "INFO:root:20/01/26 21:35:54 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:35:54 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074553357\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0007/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:35:55 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:35:56 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:35:57 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:35:58 INFO Client: Application report for application_1580072780075_0007 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:35:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0007), /proxy/application_1580072780075_0007\n",
      "INFO:root:20/01/26 21:35:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "INFO:root:20/01/26 21:35:59 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/26 21:35:59 INFO Client: Application report for application_1580072780075_0007 (state: RUNNING)\n",
      "INFO:root:20/01/26 21:35:59 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.114\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074553357\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0007/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:35:59 INFO YarnClientSchedulerBackend: Application application_1580072780075_0007 has started running.\n",
      "INFO:root:20/01/26 21:35:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35023.\n",
      "INFO:root:20/01/26 21:35:59 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:35023\n",
      "INFO:root:20/01/26 21:35:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/26 21:35:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "INFO:root:20/01/26 21:35:59 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:35023 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "INFO:root:20/01/26 21:35:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "INFO:root:20/01/26 21:35:59 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/26 21:35:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 35023, None)\n",
      "INFO:root:20/01/26 21:35:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "INFO:root:20/01/26 21:36:00 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0007\n",
      "INFO:root:20/01/26 21:36:00 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/26 21:36:00 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "INFO:root:20/01/26 21:36:00 INFO SparkEntries: Spark context finished initialization in 19492ms\n",
      "INFO:root:20/01/26 21:36:00 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "INFO:root:20/01/26 21:36:03 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:40034) with ID 1\n",
      "INFO:root:20/01/26 21:36:03 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)\n",
      "INFO:root:20/01/26 21:36:04 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:33533 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-114.ec2.internal, 33533, None)\n",
      "INFO:root:20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:35374) with ID 3\n",
      "INFO:root:20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 2)\n",
      "INFO:root:20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:53966) with ID 5\n",
      "INFO:root:20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 3)\n",
      "INFO:root:20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:53968) with ID 4\n",
      "INFO:root:20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 4)\n",
      "INFO:root:20/01/26 21:36:08 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:35376) with ID 2\n",
      "INFO:root:20/01/26 21:36:08 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 5)\n",
      "INFO:root:20/01/26 21:36:08 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:41525 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-35.ec2.internal, 41525, None)\n",
      "INFO:root:20/01/26 21:36:08 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:42911 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-244.ec2.internal, 42911, None)\n",
      "INFO:root:20/01/26 21:36:08 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:44005 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-244.ec2.internal, 44005, None)\n",
      "INFO:root:20/01/26 21:36:09 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:32831 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-35.ec2.internal, 32831, None)\n",
      "INFO:root:20/01/26 21:36:12 INFO SparkEntries: Created HiveContext.\n",
      "INFO:root:20/01/26 21:36:21 WARN DAG: 0/7 - total rows in this batch: 1714\n",
      "INFO:root:20/01/26 21:37:45 WARN DAG: done!\n",
      "INFO:root:20/01/26 21:37:46 WARN DAG: 0/7 - total rows in this batch: 1713\n",
      "INFO:root:20/01/26 21:39:05 WARN DAG: done!\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests.py',\n",
    "        args=args_si,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_p = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'YESTERDAY_DATE': '2020-12-10',\n",
    "    'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "    'STOCKS': STOCKS,\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "    'URL': \"http://app.quotemedia.com/quotetools/getHistoryDownload.csv?&webmasterId=501&startDay={sd}&startMonth={sm}&startYear={sy}&endDay={ed}&endMonth={em}&endYear={ey}&isRanged=true&symbol={sym}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Sent spark session creation command to http://ec2-100-26-146-164.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Sun, 26 Jan 2020 21:39:32 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/7', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:{'id': 7, 'name': None, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Sun, 26 Jan 2020 21:39:32 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/7', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Sun, 26 Jan 2020 21:40:13 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/7/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.7547169811320755, 'state': 'running'}\n",
      "INFO:root:Progress: 0.7547169811320755\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.9152542372881356, 'state': 'running'}\n",
      "INFO:root:Progress: 0.9152542372881356\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': ''}, 'execution_count': 0, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/26 21:39:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/26 21:39:46 INFO Client: Uploading resource file:/mnt/tmp/spark-6ab3597a-7f52-423c-8616-23c384c47f47/__spark_libs__7100437802341360159.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/__spark_libs__7100437802341360159.zip\n",
      "20/01/26 21:39:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-api-0.6.0-incubating.jar\n",
      "20/01/26 21:39:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/netty-all-4.1.17.Final.jar\n",
      "20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-rsc-0.6.0-incubating.jar\n",
      "20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/commons-codec-1.9.jar\n",
      "20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-core_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-repl_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:39:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/26 21:39:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/com.epam_parso-2.0.8.jar\n",
      "20/01/26 21:39:50 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/26 21:39:50 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/26 21:39:50 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/26 21:39:51 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/hive-site.xml\n",
      "20/01/26 21:39:51 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/sparkr.zip\n",
      "20/01/26 21:39:52 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/pyspark.zip\n",
      "20/01/26 21:39:52 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/py4j-0.10.7-src.zip\n",
      "20/01/26 21:39:53 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/01/26 21:39:53 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:39:53 INFO Client: Uploading resource file:/mnt/tmp/spark-6ab3597a-7f52-423c-8616-23c384c47f47/__spark_conf__203099066307077020.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/__spark_conf__.zip\n",
      "20/01/26 21:39:53 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/26 21:39:53 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/26 21:39:53 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/26 21:39:53 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/26 21:39:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/26 21:39:54 INFO Client: Submitting application application_1580072780075_0008 to ResourceManager\n",
      "20/01/26 21:39:54 INFO YarnClientImpl: Submitted application application_1580072780075_0008\n",
      "20/01/26 21:39:54 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0008 and attemptId None\n",
      "20/01/26 21:39:55 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "20/01/26 21:39:55 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074794631\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0008/\n",
      "\t user: livy\n",
      "20/01/26 21:39:56 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "20/01/26 21:39:57 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "20/01/26 21:39:58 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "20/01/26 21:39:59 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "20/01/26 21:40:00 INFO Client: Application report for application_1580072780075_0008 (state: RUNNING)\n",
      "20/01/26 21:40:00 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.244\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074794631\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0008/\n",
      "\t user: livy\n",
      "20/01/26 21:40:00 INFO YarnClientSchedulerBackend: Application application_1580072780075_0008 has started running.\n",
      "20/01/26 21:40:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34937.\n",
      "20/01/26 21:40:00 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:34937\n",
      "20/01/26 21:40:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/26 21:40:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "20/01/26 21:40:00 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:34937 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "20/01/26 21:40:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "20/01/26 21:40:00 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/26 21:40:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "20/01/26 21:40:01 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0008), /proxy/application_1580072780075_0008\n",
      "20/01/26 21:40:01 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/01/26 21:40:01 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/01/26 21:40:01 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/26 21:40:01 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0008\n",
      "20/01/26 21:40:01 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/26 21:40:01 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/01/26 21:40:01 INFO SparkEntries: Spark context finished initialization in 19914ms\n",
      "20/01/26 21:40:01 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "20/01/26 21:40:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:40470) with ID 5\n",
      "20/01/26 21:40:06 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 1)\n",
      "20/01/26 21:40:06 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:35757 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-244.ec2.internal, 35757, None)\n",
      "20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:45576) with ID 3\n",
      "20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 2)\n",
      "20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:46796) with ID 1\n",
      "20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 3)\n",
      "20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:46798) with ID 2\n",
      "20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 4)\n",
      "20/01/26 21:40:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:46255 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-35.ec2.internal, 46255, None)\n",
      "20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:45578) with ID 4\n",
      "20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 5)\n",
      "20/01/26 21:40:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:42823 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-114.ec2.internal, 42823, None)\n",
      "20/01/26 21:40:11 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:34665 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-114.ec2.internal, 34665, None)\n",
      "20/01/26 21:40:11 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:45475 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-35.ec2.internal, 45475, None)\n",
      "20/01/26 21:40:14 INFO SparkEntries: Created HiveContext.\n",
      "20/01/26 21:40:16 WARN DAG: HELLO1\n",
      "20/01/26 21:40:16 WARN DAG: HELLO2\n",
      "20/01/26 21:40:21 WARN DAG: Creating table s3a://short-interest-effect/data/raw/prices\n",
      "20/01/26 21:40:21 WARN DAG:     Creating temporary table s3a://short-interest-effect/data/raw/prices-temp\n",
      "20/01/26 21:40:36 WARN DAG:     done! Now creating table s3a://short-interest-effect/data/raw/prices\n",
      "20/01/26 21:40:46 WARN DAG: done!\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/26 21:39:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/26 21:39:46 INFO Client: Uploading resource file:/mnt/tmp/spark-6ab3597a-7f52-423c-8616-23c384c47f47/__spark_libs__7100437802341360159.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/__spark_libs__7100437802341360159.zip\n",
      "INFO:root:20/01/26 21:39:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-api-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:39:47 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/netty-all-4.1.17.Final.jar\n",
      "INFO:root:20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-rsc-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/commons-codec-1.9.jar\n",
      "INFO:root:20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-core_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:39:48 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/livy-repl_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:39:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/26 21:39:49 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/26 21:39:50 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/26 21:39:50 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/26 21:39:50 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/26 21:39:51 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/hive-site.xml\n",
      "INFO:root:20/01/26 21:39:51 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/sparkr.zip\n",
      "INFO:root:20/01/26 21:39:52 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/pyspark.zip\n",
      "INFO:root:20/01/26 21:39:52 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/py4j-0.10.7-src.zip\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:39:53 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:39:53 INFO Client: Uploading resource file:/mnt/tmp/spark-6ab3597a-7f52-423c-8616-23c384c47f47/__spark_conf__203099066307077020.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0008/__spark_conf__.zip\n",
      "INFO:root:20/01/26 21:39:53 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/26 21:39:53 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/26 21:39:53 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/26 21:39:53 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/26 21:39:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/26 21:39:54 INFO Client: Submitting application application_1580072780075_0008 to ResourceManager\n",
      "INFO:root:20/01/26 21:39:54 INFO YarnClientImpl: Submitted application application_1580072780075_0008\n",
      "INFO:root:20/01/26 21:39:54 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0008 and attemptId None\n",
      "INFO:root:20/01/26 21:39:55 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:39:55 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074794631\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0008/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:39:56 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:39:57 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:39:58 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:39:59 INFO Client: Application report for application_1580072780075_0008 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:40:00 INFO Client: Application report for application_1580072780075_0008 (state: RUNNING)\n",
      "INFO:root:20/01/26 21:40:00 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.244\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074794631\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0008/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:40:00 INFO YarnClientSchedulerBackend: Application application_1580072780075_0008 has started running.\n",
      "INFO:root:20/01/26 21:40:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34937.\n",
      "INFO:root:20/01/26 21:40:00 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:34937\n",
      "INFO:root:20/01/26 21:40:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/26 21:40:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "INFO:root:20/01/26 21:40:00 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:34937 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "INFO:root:20/01/26 21:40:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "INFO:root:20/01/26 21:40:00 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/26 21:40:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 34937, None)\n",
      "INFO:root:20/01/26 21:40:01 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0008), /proxy/application_1580072780075_0008\n",
      "INFO:root:20/01/26 21:40:01 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "INFO:root:20/01/26 21:40:01 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "INFO:root:20/01/26 21:40:01 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/26 21:40:01 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0008\n",
      "INFO:root:20/01/26 21:40:01 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/26 21:40:01 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "INFO:root:20/01/26 21:40:01 INFO SparkEntries: Spark context finished initialization in 19914ms\n",
      "INFO:root:20/01/26 21:40:01 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "INFO:root:20/01/26 21:40:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:40470) with ID 5\n",
      "INFO:root:20/01/26 21:40:06 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 1)\n",
      "INFO:root:20/01/26 21:40:06 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:35757 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-244.ec2.internal, 35757, None)\n",
      "INFO:root:20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:45576) with ID 3\n",
      "INFO:root:20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 2)\n",
      "INFO:root:20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:46796) with ID 1\n",
      "INFO:root:20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 3)\n",
      "INFO:root:20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:46798) with ID 2\n",
      "INFO:root:20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 4)\n",
      "INFO:root:20/01/26 21:40:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:46255 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-35.ec2.internal, 46255, None)\n",
      "INFO:root:20/01/26 21:40:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:45578) with ID 4\n",
      "INFO:root:20/01/26 21:40:10 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 5)\n",
      "INFO:root:20/01/26 21:40:10 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:42823 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-114.ec2.internal, 42823, None)\n",
      "INFO:root:20/01/26 21:40:11 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:34665 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-114.ec2.internal, 34665, None)\n",
      "INFO:root:20/01/26 21:40:11 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:45475 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-35.ec2.internal, 45475, None)\n",
      "INFO:root:20/01/26 21:40:14 INFO SparkEntries: Created HiveContext.\n",
      "INFO:root:20/01/26 21:40:16 WARN DAG: HELLO1\n",
      "INFO:root:20/01/26 21:40:16 WARN DAG: HELLO2\n",
      "INFO:root:20/01/26 21:40:21 WARN DAG: Creating table s3a://short-interest-effect/data/raw/prices\n",
      "INFO:root:20/01/26 21:40:21 WARN DAG:     Creating temporary table s3a://short-interest-effect/data/raw/prices-temp\n",
      "INFO:root:20/01/26 21:40:36 WARN DAG:     done! Now creating table s3a://short-interest-effect/data/raw/prices\n",
      "INFO:root:20/01/26 21:40:46 WARN DAG: done!\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices.py',\n",
    "        args=args_p,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_q = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'STOCKS': STOCKS,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "            'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Sent spark session creation command to http://ec2-100-26-146-164.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Sun, 26 Jan 2020 21:41:01 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/8', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:{'id': 8, 'name': None, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Sun, 26 Jan 2020 21:41:01 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/8', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Sun, 26 Jan 2020 21:41:36 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/8/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: waiting\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.42857142857142855, 'state': 'running'}\n",
      "INFO:root:Progress: 0.42857142857142855\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': ''}, 'execution_count': 0, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/26 21:41:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "20/01/26 21:41:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/01/26 21:41:12 INFO Client: Setting up container launch context for our AM\n",
      "20/01/26 21:41:12 INFO Client: Setting up the launch environment for our AM container\n",
      "20/01/26 21:41:12 INFO Client: Preparing resources for our AM container\n",
      "20/01/26 21:41:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/26 21:41:14 INFO Client: Uploading resource file:/mnt/tmp/spark-2f55fa94-5d6b-4cfe-8038-2830fa9d0eae/__spark_libs__8774946579663814247.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/__spark_libs__8774946579663814247.zip\n",
      "20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-api-0.6.0-incubating.jar\n",
      "20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/netty-all-4.1.17.Final.jar\n",
      "20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-rsc-0.6.0-incubating.jar\n",
      "20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/commons-codec-1.9.jar\n",
      "20/01/26 21:41:17 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-core_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:41:17 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-repl_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:41:17 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/com.epam_parso-2.0.8.jar\n",
      "20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/26 21:41:18 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/hive-site.xml\n",
      "20/01/26 21:41:19 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/sparkr.zip\n",
      "20/01/26 21:41:19 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/pyspark.zip\n",
      "20/01/26 21:41:19 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/py4j-0.10.7-src.zip\n",
      "20/01/26 21:41:20 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/01/26 21:41:20 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:41:20 INFO Client: Uploading resource file:/mnt/tmp/spark-2f55fa94-5d6b-4cfe-8038-2830fa9d0eae/__spark_conf__8625737470097364696.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/__spark_conf__.zip\n",
      "20/01/26 21:41:20 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/26 21:41:20 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/26 21:41:20 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/26 21:41:20 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/26 21:41:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/26 21:41:21 INFO Client: Submitting application application_1580072780075_0009 to ResourceManager\n",
      "20/01/26 21:41:21 INFO YarnClientImpl: Submitted application application_1580072780075_0009\n",
      "20/01/26 21:41:21 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0009 and attemptId None\n",
      "20/01/26 21:41:23 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "20/01/26 21:41:23 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074881956\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0009/\n",
      "\t user: livy\n",
      "20/01/26 21:41:24 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "20/01/26 21:41:25 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "20/01/26 21:41:26 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "20/01/26 21:41:27 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "20/01/26 21:41:27 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0009), /proxy/application_1580072780075_0009\n",
      "20/01/26 21:41:27 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/01/26 21:41:27 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/26 21:41:28 INFO Client: Application report for application_1580072780075_0009 (state: RUNNING)\n",
      "20/01/26 21:41:28 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.244\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074881956\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0009/\n",
      "\t user: livy\n",
      "20/01/26 21:41:28 INFO YarnClientSchedulerBackend: Application application_1580072780075_0009 has started running.\n",
      "20/01/26 21:41:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42939.\n",
      "20/01/26 21:41:28 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:42939\n",
      "20/01/26 21:41:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/26 21:41:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "20/01/26 21:41:28 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:42939 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "20/01/26 21:41:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "20/01/26 21:41:28 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/26 21:41:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "20/01/26 21:41:28 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/01/26 21:41:28 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0009\n",
      "20/01/26 21:41:28 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/26 21:41:28 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/01/26 21:41:28 INFO SparkEntries: Spark context finished initialization in 18858ms\n",
      "20/01/26 21:41:29 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "20/01/26 21:41:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:40102) with ID 3\n",
      "20/01/26 21:41:32 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 1)\n",
      "20/01/26 21:41:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:39953 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-244.ec2.internal, 39953, None)\n",
      "20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:50566) with ID 2\n",
      "20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)\n",
      "20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:38076) with ID 4\n",
      "20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 3)\n",
      "20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:50568) with ID 1\n",
      "20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 4)\n",
      "20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:38078) with ID 5\n",
      "20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 5)\n",
      "20/01/26 21:41:35 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:46129 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-114.ec2.internal, 46129, None)\n",
      "20/01/26 21:41:35 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:38069 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-114.ec2.internal, 38069, None)\n",
      "20/01/26 21:41:36 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:41041 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-35.ec2.internal, 41041, None)\n",
      "20/01/26 21:41:36 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:34603 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-35.ec2.internal, 34603, None)\n",
      "20/01/26 21:41:37 INFO SparkEntries: Created HiveContext.\n",
      "20/01/26 21:41:49 WARN DAG: (SUCCESS) Table s3a://short-interest-effect/data/raw/prices has 10651 rows.\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/26 21:41:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "INFO:root:20/01/26 21:41:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "INFO:root:20/01/26 21:41:12 INFO Client: Setting up container launch context for our AM\n",
      "INFO:root:20/01/26 21:41:12 INFO Client: Setting up the launch environment for our AM container\n",
      "INFO:root:20/01/26 21:41:12 INFO Client: Preparing resources for our AM container\n",
      "INFO:root:20/01/26 21:41:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/26 21:41:14 INFO Client: Uploading resource file:/mnt/tmp/spark-2f55fa94-5d6b-4cfe-8038-2830fa9d0eae/__spark_libs__8774946579663814247.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/__spark_libs__8774946579663814247.zip\n",
      "INFO:root:20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-api-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/netty-all-4.1.17.Final.jar\n",
      "INFO:root:20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-rsc-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:41:16 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/commons-codec-1.9.jar\n",
      "INFO:root:20/01/26 21:41:17 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-core_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:41:17 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/livy-repl_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:41:17 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/26 21:41:18 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/26 21:41:18 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/hive-site.xml\n",
      "INFO:root:20/01/26 21:41:19 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/sparkr.zip\n",
      "INFO:root:20/01/26 21:41:19 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/pyspark.zip\n",
      "INFO:root:20/01/26 21:41:19 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/py4j-0.10.7-src.zip\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:41:20 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:41:20 INFO Client: Uploading resource file:/mnt/tmp/spark-2f55fa94-5d6b-4cfe-8038-2830fa9d0eae/__spark_conf__8625737470097364696.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0009/__spark_conf__.zip\n",
      "INFO:root:20/01/26 21:41:20 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/26 21:41:20 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/26 21:41:20 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/26 21:41:20 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/26 21:41:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/26 21:41:21 INFO Client: Submitting application application_1580072780075_0009 to ResourceManager\n",
      "INFO:root:20/01/26 21:41:21 INFO YarnClientImpl: Submitted application application_1580072780075_0009\n",
      "INFO:root:20/01/26 21:41:21 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0009 and attemptId None\n",
      "INFO:root:20/01/26 21:41:23 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:41:23 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074881956\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0009/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:41:24 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:41:25 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:41:26 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:41:27 INFO Client: Application report for application_1580072780075_0009 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:41:27 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0009), /proxy/application_1580072780075_0009\n",
      "INFO:root:20/01/26 21:41:27 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "INFO:root:20/01/26 21:41:27 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/26 21:41:28 INFO Client: Application report for application_1580072780075_0009 (state: RUNNING)\n",
      "INFO:root:20/01/26 21:41:28 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.244\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074881956\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0009/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:41:28 INFO YarnClientSchedulerBackend: Application application_1580072780075_0009 has started running.\n",
      "INFO:root:20/01/26 21:41:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42939.\n",
      "INFO:root:20/01/26 21:41:28 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:42939\n",
      "INFO:root:20/01/26 21:41:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/26 21:41:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "INFO:root:20/01/26 21:41:28 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:42939 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "INFO:root:20/01/26 21:41:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "INFO:root:20/01/26 21:41:28 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/26 21:41:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 42939, None)\n",
      "INFO:root:20/01/26 21:41:28 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "INFO:root:20/01/26 21:41:28 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0009\n",
      "INFO:root:20/01/26 21:41:28 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/26 21:41:28 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "INFO:root:20/01/26 21:41:28 INFO SparkEntries: Spark context finished initialization in 18858ms\n",
      "INFO:root:20/01/26 21:41:29 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "INFO:root:20/01/26 21:41:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:40102) with ID 3\n",
      "INFO:root:20/01/26 21:41:32 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 1)\n",
      "INFO:root:20/01/26 21:41:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:39953 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-244.ec2.internal, 39953, None)\n",
      "INFO:root:20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:50566) with ID 2\n",
      "INFO:root:20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)\n",
      "INFO:root:20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:38076) with ID 4\n",
      "INFO:root:20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 3)\n",
      "INFO:root:20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:50568) with ID 1\n",
      "INFO:root:20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 4)\n",
      "INFO:root:20/01/26 21:41:35 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:38078) with ID 5\n",
      "INFO:root:20/01/26 21:41:35 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 5)\n",
      "INFO:root:20/01/26 21:41:35 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:46129 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-114.ec2.internal, 46129, None)\n",
      "INFO:root:20/01/26 21:41:35 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:38069 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-114.ec2.internal, 38069, None)\n",
      "INFO:root:20/01/26 21:41:36 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:41041 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-35.ec2.internal, 41041, None)\n",
      "INFO:root:20/01/26 21:41:36 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:34603 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-35.ec2.internal, 34603, None)\n",
      "INFO:root:20/01/26 21:41:37 INFO SparkEntries: Created HiveContext.\n",
      "INFO:root:20/01/26 21:41:49 WARN DAG: (SUCCESS) Table s3a://short-interest-effect/data/raw/prices has 10651 rows.\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_prices_quality.py',\n",
    "        args=args_q,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_c = {\n",
    "            'YESTERDAY_DATE': '2020-12-10',\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_STOCK_PRICES': config['App']['TABLE_STOCK_PRICES'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Killing all inactive spark sessions\n",
      "INFO:root:Sent spark session creation command to http://ec2-100-26-146-164.compute-1.amazonaws.com:8998/sessions\n",
      "INFO:root:Response headers: {'Date': 'Sun, 26 Jan 2020 21:41:56 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/9', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:{'id': 9, 'name': None, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'starting', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['stdout: ', '\\nstderr: ', '\\nYARN Diagnostics: ']}\n",
      "INFO:root:Session headers: {'Date': 'Sun, 26 Jan 2020 21:41:56 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/9', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: starting\n",
      "INFO:root:Spark session status: idle\n",
      "INFO:root:Spark job sending successful:\n",
      "Response status code: 201\n",
      "Headers: {'Date': 'Sun, 26 Jan 2020 21:42:36 GMT', 'Content-Type': 'application/json;charset=utf-8', 'Content-Encoding': 'gzip', 'Location': '/sessions/9/statements/0', 'Transfer-Encoding': 'chunked', 'Server': 'Jetty(9.3.24.v20180605)'}\n",
      "Content: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Spark Job status: waiting\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'waiting'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.0, 'state': 'running'}\n",
      "INFO:root:Progress: 0.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.99644128113879, 'state': 'running'}\n",
      "INFO:root:Progress: 0.99644128113879\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.9634551495016611, 'state': 'running'}\n",
      "INFO:root:Progress: 0.9634551495016611\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.6015625, 'state': 'running'}\n",
      "INFO:root:Progress: 0.6015625\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.7021857923497268, 'state': 'running'}\n",
      "INFO:root:Progress: 0.7021857923497268\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.796448087431694, 'state': 'running'}\n",
      "INFO:root:Progress: 0.796448087431694\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 0.9562841530054644, 'state': 'running'}\n",
      "INFO:root:Progress: 0.9562841530054644\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: running\n",
      "INFO:root:Response: {'id': 0, 'output': None, 'progress': 1.0, 'state': 'running'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Spark Job status: available\n",
      "INFO:root:Response: {'id': 0,\n",
      " 'output': {'data': {'text/plain': ''}, 'execution_count': 0, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'state': 'available'}\n",
      "INFO:root:Progress: 1.0\n",
      "INFO:root:Log from the cluster:\n",
      "20/01/26 21:42:07 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "20/01/26 21:42:07 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/01/26 21:42:07 INFO Client: Setting up container launch context for our AM\n",
      "20/01/26 21:42:07 INFO Client: Setting up the launch environment for our AM container\n",
      "20/01/26 21:42:07 INFO Client: Preparing resources for our AM container\n",
      "20/01/26 21:42:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "20/01/26 21:42:10 INFO Client: Uploading resource file:/mnt/tmp/spark-b4c695be-2754-4fab-9aba-15fcead7156f/__spark_libs__4470983914913100288.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/__spark_libs__4470983914913100288.zip\n",
      "20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-api-0.6.0-incubating.jar\n",
      "20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/netty-all-4.1.17.Final.jar\n",
      "20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-rsc-0.6.0-incubating.jar\n",
      "20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/commons-codec-1.9.jar\n",
      "20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-core_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:42:13 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-repl_2.11-0.6.0-incubating.jar\n",
      "20/01/26 21:42:13 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "20/01/26 21:42:14 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/com.epam_parso-2.0.8.jar\n",
      "20/01/26 21:42:14 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "20/01/26 21:42:14 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/org.slf4j_slf4j-api-1.7.5.jar\n",
      "20/01/26 21:42:15 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "20/01/26 21:42:15 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/hive-site.xml\n",
      "20/01/26 21:42:15 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/sparkr.zip\n",
      "20/01/26 21:42:16 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/pyspark.zip\n",
      "20/01/26 21:42:16 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/py4j-0.10.7-src.zip\n",
      "20/01/26 21:42:17 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "20/01/26 21:42:17 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "20/01/26 21:42:17 INFO Client: Uploading resource file:/mnt/tmp/spark-b4c695be-2754-4fab-9aba-15fcead7156f/__spark_conf__7279100698985417908.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/__spark_conf__.zip\n",
      "20/01/26 21:42:17 INFO SecurityManager: Changing view acls to: livy\n",
      "20/01/26 21:42:17 INFO SecurityManager: Changing modify acls to: livy\n",
      "20/01/26 21:42:17 INFO SecurityManager: Changing view acls groups to: \n",
      "20/01/26 21:42:17 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/01/26 21:42:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "20/01/26 21:42:18 INFO Client: Submitting application application_1580072780075_0010 to ResourceManager\n",
      "20/01/26 21:42:19 INFO YarnClientImpl: Submitted application application_1580072780075_0010\n",
      "20/01/26 21:42:19 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0010 and attemptId None\n",
      "20/01/26 21:42:20 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "20/01/26 21:42:20 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074938880\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0010/\n",
      "\t user: livy\n",
      "20/01/26 21:42:21 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "20/01/26 21:42:22 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "20/01/26 21:42:23 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "20/01/26 21:42:24 INFO Client: Application report for application_1580072780075_0010 (state: RUNNING)\n",
      "20/01/26 21:42:24 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.16.2.244\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1580074938880\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0010/\n",
      "\t user: livy\n",
      "20/01/26 21:42:24 INFO YarnClientSchedulerBackend: Application application_1580072780075_0010 has started running.\n",
      "20/01/26 21:42:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44911.\n",
      "20/01/26 21:42:24 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:44911\n",
      "20/01/26 21:42:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/01/26 21:42:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "20/01/26 21:42:24 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:44911 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "20/01/26 21:42:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "20/01/26 21:42:24 INFO BlockManager: external shuffle service port = 7337\n",
      "20/01/26 21:42:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "20/01/26 21:42:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0010), /proxy/application_1580072780075_0010\n",
      "20/01/26 21:42:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/01/26 21:42:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/01/26 21:42:24 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/01/26 21:42:24 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0010\n",
      "20/01/26 21:42:24 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/01/26 21:42:24 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/01/26 21:42:24 INFO SparkEntries: Spark context finished initialization in 18968ms\n",
      "20/01/26 21:42:24 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "20/01/26 21:42:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:53388) with ID 3\n",
      "20/01/26 21:42:29 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 1)\n",
      "20/01/26 21:42:29 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:44227 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-244.ec2.internal, 44227, None)\n",
      "20/01/26 21:42:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:51748) with ID 5\n",
      "20/01/26 21:42:31 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 2)\n",
      "20/01/26 21:42:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:51750) with ID 4\n",
      "20/01/26 21:42:31 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 3)\n",
      "20/01/26 21:42:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:55518) with ID 1\n",
      "20/01/26 21:42:32 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 4)\n",
      "20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:38437 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-114.ec2.internal, 38437, None)\n",
      "20/01/26 21:42:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:55520) with ID 2\n",
      "20/01/26 21:42:32 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 5)\n",
      "20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:40807 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-114.ec2.internal, 40807, None)\n",
      "20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:39463 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-35.ec2.internal, 39463, None)\n",
      "20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:46351 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-35.ec2.internal, 46351, None)\n",
      "20/01/26 21:42:36 INFO SparkEntries: Created HiveContext.\n",
      "20/01/26 21:42:55 WARN DAG: Creating table s3a://short-interest-effect/data/processed/short_analysis\n",
      "20/01/26 21:44:48 WARN DAG: done!\n",
      "\n",
      "YARN Diagnostics: \n",
      "INFO:root:Final job Status: ok\n",
      "INFO:root:20/01/26 21:42:07 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "INFO:root:20/01/26 21:42:07 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "INFO:root:20/01/26 21:42:07 INFO Client: Setting up container launch context for our AM\n",
      "INFO:root:20/01/26 21:42:07 INFO Client: Setting up the launch environment for our AM container\n",
      "INFO:root:20/01/26 21:42:07 INFO Client: Preparing resources for our AM container\n",
      "INFO:root:20/01/26 21:42:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "INFO:root:20/01/26 21:42:10 INFO Client: Uploading resource file:/mnt/tmp/spark-b4c695be-2754-4fab-9aba-15fcead7156f/__spark_libs__4470983914913100288.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/__spark_libs__4470983914913100288.zip\n",
      "INFO:root:20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-api-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/netty-all-4.1.17.Final.jar\n",
      "INFO:root:20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-rsc-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/commons-codec-1.9.jar\n",
      "INFO:root:20/01/26 21:42:12 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-core_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:42:13 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/livy-repl_2.11-0.6.0-incubating.jar\n",
      "INFO:root:20/01/26 21:42:13 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar\n",
      "INFO:root:20/01/26 21:42:14 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/com.epam_parso-2.0.8.jar\n",
      "INFO:root:20/01/26 21:42:14 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar\n",
      "INFO:root:20/01/26 21:42:14 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/org.slf4j_slf4j-api-1.7.5.jar\n",
      "INFO:root:20/01/26 21:42:15 INFO Client: Uploading resource file:/var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/org.scala-lang_scala-reflect-2.11.8.jar\n",
      "INFO:root:20/01/26 21:42:15 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/hive-site.xml\n",
      "INFO:root:20/01/26 21:42:15 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/sparkr.zip\n",
      "INFO:root:20/01/26 21:42:16 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/pyspark.zip\n",
      "INFO:root:20/01/26 21:42:16 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/py4j-0.10.7-src.zip\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same name resource file:///usr/lib/spark/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same name resource file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/com.epam_parso-2.0.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:42:17 WARN Client: Same path resource file:///var/lib/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\n",
      "INFO:root:20/01/26 21:42:17 INFO Client: Uploading resource file:/mnt/tmp/spark-b4c695be-2754-4fab-9aba-15fcead7156f/__spark_conf__7279100698985417908.zip -> hdfs://ip-172-16-2-34.ec2.internal:8020/user/livy/.sparkStaging/application_1580072780075_0010/__spark_conf__.zip\n",
      "INFO:root:20/01/26 21:42:17 INFO SecurityManager: Changing view acls to: livy\n",
      "INFO:root:20/01/26 21:42:17 INFO SecurityManager: Changing modify acls to: livy\n",
      "INFO:root:20/01/26 21:42:17 INFO SecurityManager: Changing view acls groups to: \n",
      "INFO:root:20/01/26 21:42:17 INFO SecurityManager: Changing modify acls groups to: \n",
      "INFO:root:20/01/26 21:42:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "INFO:root:20/01/26 21:42:18 INFO Client: Submitting application application_1580072780075_0010 to ResourceManager\n",
      "INFO:root:20/01/26 21:42:19 INFO YarnClientImpl: Submitted application application_1580072780075_0010\n",
      "INFO:root:20/01/26 21:42:19 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1580072780075_0010 and attemptId None\n",
      "INFO:root:20/01/26 21:42:20 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:42:20 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "INFO:root:\t ApplicationMaster host: N/A\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074938880\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0010/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:42:21 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:42:22 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:42:23 INFO Client: Application report for application_1580072780075_0010 (state: ACCEPTED)\n",
      "INFO:root:20/01/26 21:42:24 INFO Client: Application report for application_1580072780075_0010 (state: RUNNING)\n",
      "INFO:root:20/01/26 21:42:24 INFO Client: \n",
      "INFO:root:\t client token: N/A\n",
      "INFO:root:\t diagnostics: N/A\n",
      "INFO:root:\t ApplicationMaster host: 172.16.2.244\n",
      "INFO:root:\t ApplicationMaster RPC port: -1\n",
      "INFO:root:\t queue: default\n",
      "INFO:root:\t start time: 1580074938880\n",
      "INFO:root:\t final status: UNDEFINED\n",
      "INFO:root:\t tracking URL: http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0010/\n",
      "INFO:root:\t user: livy\n",
      "INFO:root:20/01/26 21:42:24 INFO YarnClientSchedulerBackend: Application application_1580072780075_0010 has started running.\n",
      "INFO:root:20/01/26 21:42:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44911.\n",
      "INFO:root:20/01/26 21:42:24 INFO NettyBlockTransferService: Server created on ip-172-16-2-34.ec2.internal:44911\n",
      "INFO:root:20/01/26 21:42:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "INFO:root:20/01/26 21:42:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "INFO:root:20/01/26 21:42:24 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-34.ec2.internal:44911 with 912.3 MB RAM, BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "INFO:root:20/01/26 21:42:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "INFO:root:20/01/26 21:42:24 INFO BlockManager: external shuffle service port = 7337\n",
      "INFO:root:20/01/26 21:42:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-16-2-34.ec2.internal, 44911, None)\n",
      "INFO:root:20/01/26 21:42:24 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-16-2-34.ec2.internal, PROXY_URI_BASES -> http://ip-172-16-2-34.ec2.internal:20888/proxy/application_1580072780075_0010), /proxy/application_1580072780075_0010\n",
      "INFO:root:20/01/26 21:42:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "INFO:root:20/01/26 21:42:24 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "INFO:root:20/01/26 21:42:24 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "INFO:root:20/01/26 21:42:24 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1580072780075_0010\n",
      "INFO:root:20/01/26 21:42:24 INFO Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "INFO:root:20/01/26 21:42:24 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "INFO:root:20/01/26 21:42:24 INFO SparkEntries: Spark context finished initialization in 18968ms\n",
      "INFO:root:20/01/26 21:42:24 INFO SparkEntries: Created Spark session (with Hive support).\n",
      "INFO:root:20/01/26 21:42:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.244:53388) with ID 3\n",
      "INFO:root:20/01/26 21:42:29 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 1)\n",
      "INFO:root:20/01/26 21:42:29 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-244.ec2.internal:44227 with 2.6 GB RAM, BlockManagerId(3, ip-172-16-2-244.ec2.internal, 44227, None)\n",
      "INFO:root:20/01/26 21:42:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:51748) with ID 5\n",
      "INFO:root:20/01/26 21:42:31 INFO ExecutorAllocationManager: New executor 5 has registered (new total is 2)\n",
      "INFO:root:20/01/26 21:42:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.114:51750) with ID 4\n",
      "INFO:root:20/01/26 21:42:31 INFO ExecutorAllocationManager: New executor 4 has registered (new total is 3)\n",
      "INFO:root:20/01/26 21:42:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:55518) with ID 1\n",
      "INFO:root:20/01/26 21:42:32 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 4)\n",
      "INFO:root:20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:38437 with 2.6 GB RAM, BlockManagerId(5, ip-172-16-2-114.ec2.internal, 38437, None)\n",
      "INFO:root:20/01/26 21:42:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.2.35:55520) with ID 2\n",
      "INFO:root:20/01/26 21:42:32 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 5)\n",
      "INFO:root:20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-114.ec2.internal:40807 with 2.6 GB RAM, BlockManagerId(4, ip-172-16-2-114.ec2.internal, 40807, None)\n",
      "INFO:root:20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:39463 with 2.6 GB RAM, BlockManagerId(1, ip-172-16-2-35.ec2.internal, 39463, None)\n",
      "INFO:root:20/01/26 21:42:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-16-2-35.ec2.internal:46351 with 2.6 GB RAM, BlockManagerId(2, ip-172-16-2-35.ec2.internal, 46351, None)\n",
      "INFO:root:20/01/26 21:42:36 INFO SparkEntries: Created HiveContext.\n",
      "INFO:root:20/01/26 21:42:55 WARN DAG: Creating table s3a://short-interest-effect/data/processed/short_analysis\n",
      "INFO:root:20/01/26 21:44:48 WARN DAG: done!\n",
      "INFO:root:\n",
      "YARN Diagnostics: \n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_inactive_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine.py',\n",
    "        args=args_c,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-3KRMX1NGNM7ZU has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-3KRMX1NGNM7ZU has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-3KRMX1NGNM7ZU has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-3KRMX1NGNM7ZU has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n",
      "INFO:root:Cluster j-3KRMX1NGNM7ZU has not been terminated (Current cluster state: TERMINATING). waiting until the status is TERMINATED...\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Resetting dropped connection: elasticmapreduce.us-east-1.amazonaws.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster j-3KRMX1NGNM7ZU Deleted\n"
     ]
    }
   ],
   "source": [
    "emrs.delete_cluster(emr, cluster_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
